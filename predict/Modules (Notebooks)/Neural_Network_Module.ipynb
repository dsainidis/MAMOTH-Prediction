{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d653348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6d570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(target,num):\n",
    "    \"\"\"Creates an one hot encoded vector based on the class of each observation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target : pd.Series\n",
    "        The series containing the target variable to be transformed to vector\n",
    "        \n",
    "    num_classes : int\n",
    "        The number of different classes\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    vectors : pd.Series\n",
    "        A pd.Series object with one hot encoded vectors\n",
    "\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(sparse=False,categories = [list(range(num))], handle_unknown='ignore')\n",
    "    enc.fit(np.array(target).reshape(-1, 1))\n",
    "    target= target.apply(lambda x: enc.transform(np.array(x).reshape(1, -1))[0])\n",
    "    target = np.stack(target)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4844c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations_nn(data, model_type, test=None, evaluation=False, transformation_list=['scaling'],\n",
    "                    embedding_data=None):\n",
    "    \"\"\"Executes scaling, augmentation or label endconding on the dataset \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        A dataframe containing all the data or the data of trainning.\n",
    "        If test not given, then data will be randomnly slpit in 80-20% train and test set\n",
    "        \n",
    "    model_type : str\n",
    "        The type of the model to be implemented.\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    test : dataframe, optional\n",
    "        A dataframe containing all the data for testing (default = None)\n",
    "        \n",
    "    scaling : boolean, optional\n",
    "        If True, perofrms scaling on numerical features (default = False)\n",
    "        \n",
    "    augment : boolean, optional\n",
    "        If True, augments the train data with existing observations\n",
    "        and giving greater weight on the observations with greater target value (default = False)\n",
    "        \n",
    "    embedding_data : dataframe, optional\n",
    "        A datafrane with the categorical features (default = None)\n",
    "        \n",
    "    evaluation : boolean, optional\n",
    "        If True, 20% of the observations of train set will be held for evaluation set (default = False)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    train_X: numpy array\n",
    "        A numpy array with independent variables for training\n",
    "        \n",
    "    train_y: pd.Series\n",
    "        A array with the dependent variables (target) for training\n",
    "        \n",
    "    test_X: numpy array\n",
    "        A numpy array with independent variables for test\n",
    "        \n",
    "    test_y: pd.Series\n",
    "        A array with the dependent variables (target) for test\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if test is None:    \n",
    "        X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        train_X,test_X,train_y,test_y = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    else:\n",
    "        data = data.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "        train_X, train_y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        test_X, test_y = test.iloc[:,:-1], test.iloc[:,-1]\n",
    "        \n",
    "    train_X = train_X.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    test_X = test_X.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "        \n",
    "    if evaluation:\n",
    "        train_X,eval_X,train_y,eval_y = train_test_split(train_X, train_y, test_size=0.20, random_state=1)\n",
    "        eval_X = eval_X.reset_index(drop=True)\n",
    "        eval_y = eval_y.reset_index(drop=True)\n",
    "        \n",
    "    if model_type == 'mosquito_regression':       \n",
    "        percentile = round(np.percentile(train_y, 95))\n",
    "        train_y.loc[train_y >= percentile] = percentile\n",
    "\n",
    "    if 'augmentation' in transformation_list:\n",
    "        augment_index = train_y.sample(frac=0.4, weights=train_y, random_state=1, replace=True).index\n",
    "        train_X = pd.concat([train_X,train_X.iloc[augment_index,:]]).reset_index(drop=True)\n",
    "        train_y = pd.concat([train_y,train_y[augment_index]]).reset_index(drop=True)\n",
    "    \n",
    "    if model_type == 'classification':\n",
    "        num_classes = len(pd.concat([train_y,test_y]).unique())\n",
    "        if evaluation:\n",
    "            num_classes = len(pd.concat([train_y, test_y, eval_y]).unique())\n",
    "            eval_y = vectorise(eval_y,num_classes)               \n",
    "        train_y = vectorise(train_y,num_classes)\n",
    "        test_y = vectorise(test_y,num_classes)\n",
    "\n",
    "    if embedding_data is not None:\n",
    "        embeddings = embedding_data.columns.tolist()\n",
    "        embedded_columns_train = train_X.loc[:,embeddings] #categorical columns\n",
    "        train_X = train_X.drop(columns=embeddings)\n",
    "\n",
    "        embedded_columns_test = test_X.loc[:,embeddings] #categorical columns\n",
    "        test_X = test_X.drop(columns=embeddings)\n",
    "        \n",
    "        if evaluation:\n",
    "            embedded_columns_eval = eval_X.loc[:,embeddings] #categorical columns\n",
    "            eval_X = eval_X.drop(columns=embeddings)\n",
    "            \n",
    "        label_encoder = LabelEncoder()\n",
    "        for col in embeddings:\n",
    "            label_encoder.fit(pd.concat([embedded_columns_train[col],embedded_columns_test[col]],axis=0))\n",
    "            if evaluation:\n",
    "                label_encoder.fit(pd.concat([embedded_columns_train[col],embedded_columns_test[col],embedded_columns_eval[col]],axis=0))\n",
    "                embedded_columns_eval[col] = label_encoder.transform(embedded_columns_eval[col])\n",
    "            embedded_columns_train[col] = label_encoder.transform(embedded_columns_train[col])\n",
    "            embedded_columns_test[col] = label_encoder.transform(embedded_columns_test[col])\n",
    "\n",
    "        train_X_emb = embedded_columns_train.values\n",
    "        test_X_emb = embedded_columns_test.values\n",
    "        if evaluation:\n",
    "            eval_X_emb = embedded_columns_eval.values\n",
    "            \n",
    "    if 'scaling' in transformation_list:\n",
    "        scaler = StandardScaler()\n",
    "        train_X = scaler.fit_transform(train_X)\n",
    "        test_X = scaler.transform(test_X)\n",
    "        if evaluation:\n",
    "            eval_X = scaler.transform(eval_X)\n",
    "            \n",
    "    if 'normalization' in transformation_list:\n",
    "        min_val = train_y.min()\n",
    "        max_val = train_y.max()\n",
    "        test_y = normalizeData(test_y, min_val, max_val)\n",
    "        train_y = normalizeData(train_y, min_val, max_val)\n",
    "        if evaluation:\n",
    "            eval_y = normalizeData(eval_y, min_val, max_val)\n",
    "            \n",
    "    if 'log' in transformation_list:\n",
    "        test_y = logData(test_y)\n",
    "        train_y = logData(train_y)\n",
    "        if evaluation:\n",
    "            eval_y = logData(eval_y)\n",
    "            \n",
    "    if embedding_data is not None:\n",
    "        train_X = [train_X,train_X_emb]\n",
    "        test_X = [test_X, test_X_emb]\n",
    "        if evaluation:\n",
    "            eval_X = [eval_X,eval_X_emb]\n",
    "    else:\n",
    "        train_X = [train_X]\n",
    "        test_X = [test_X]\n",
    "        if evaluation:\n",
    "            eval_X = [eval_X]\n",
    "\n",
    "            \n",
    "    if evaluation:\n",
    "        return train_X, train_y, eval_X, eval_y, test_X, test_y\n",
    "    else:       \n",
    "        return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e81874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\" Initialization of Dataset for the Neural Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            An array with the independent variables\n",
    "            \n",
    "        y : numpy array\n",
    "            An array with the dependent variables\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.emb = False\n",
    "        \n",
    "        self.X = X[0]\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float)\n",
    "        \n",
    "        if len(X)==2:\n",
    "            self.X_emb = X[1] #categorical columns\n",
    "            self.X_emb = torch.tensor(self.X_emb, dtype=torch.int)\n",
    "            self.emb = True\n",
    "        \n",
    "        self.y = y\n",
    "        if not (isinstance(y[0], list) or isinstance(y[0], np.ndarray)):\n",
    "            self.y = [[e] for e in self.y]\n",
    "            \n",
    "        self.y = torch.tensor(self.y, dtype=torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label\n",
    "        if self.emb:\n",
    "            X1 = self.X[index]\n",
    "            X2 = self.X_emb[index]\n",
    "            y = self.y[index]\n",
    "            return X1, X2, y\n",
    "        else:\n",
    "            X = self.X[index]\n",
    "            y = self.y[index]\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1b7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(y, min_val=0, max_val=9):\n",
    "    'Transforms the target data to 0-1 range in order to use the sigmoid function as activation'\n",
    "    return ((np.array(y) - min_val) / (max_val-min_val))\n",
    "\n",
    "def logData(y):\n",
    "    'Transforms the target variable to log(target) in order to follow a more normal disribution'\n",
    "    return np.log1p(y)\n",
    "\n",
    "def expData(data):\n",
    "    'Calculate the exponential value of the target variable'\n",
    "    return(np.expm1(data))\n",
    "\n",
    "def denormalizeData(data, min_val=0, max_val=9):\n",
    "    'Transforms the data from 0-1 range to the initial range'\n",
    "    return((np.array(data)*(max_val-min_val))+min_val).tolist()\n",
    "\n",
    "def sigmoid(x, a=1, b=0):\n",
    "    return 1.0 / (1.0 + np.exp(a*(-x+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7288e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    'Early stopping is a form of regularization used to avoid overfitting on the training dataset.'\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        \"\"\"Set the tolerance and the min_delta for the early stopping \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tolerance : int, optinal\n",
    "            The number of how many epochs to wait after validation score - training score\n",
    "            is greater than min delta. (default = 5)\n",
    "            \n",
    "        min delta : int, optinal\n",
    "            The threshold after which the difference of  validation score - training score\n",
    "            is critical (default = 0)        \n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop_flag = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521d48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embedding_sizes(data):\n",
    "    \"\"\"Calculates the size of the input and the output of the embedding layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array\n",
    "         An array containg the categorical features\n",
    "         \n",
    "     Returns\n",
    "     ----------\n",
    "     embedding_sizes: list\n",
    "         A list of tuples containg the input and output size for each categorical feature\n",
    "         example: [(input_size,output_size),...,(input_size,output_size)]\n",
    "    \"\"\"\n",
    "    embedding_sizes = []\n",
    "    for column in data.T:\n",
    "        n_categories = len(np.unique(column))\n",
    "        embedding_sizes.append(((n_categories, min(50, (n_categories+1)//2))))\n",
    "    return embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e637f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    'Definition of a neural Network'\n",
    "    def __init__(self, num_features, num_class, hidden_layers, model_type, learning_rate,\n",
    "                 epochs, batch_size, embedding_data=None, dropout=None, transformation_list=None,\n",
    "                 early_stop = None, l1_weight=0, l2_weight=0, weights=False):\n",
    "        \"\"\"Initilization of the layers of the neural network\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features : int\n",
    "            The number of features for input\n",
    "            \n",
    "        num_class : int\n",
    "            The number of outputs of the model\n",
    "            \n",
    "        hidden_layers : list\n",
    "            A list of int with the neurons of each layer\n",
    "\n",
    "        model_type : str\n",
    "            The type odf the model to be implemented.\n",
    "            Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "            \n",
    "        learning_rate : int\n",
    "            The learning_rate of the training process.\n",
    "        \n",
    "        epochs : int\n",
    "            The number of epochs for the training.\n",
    "\n",
    "        batch_size : int\n",
    "            The size of each batch in each iteration\n",
    "\n",
    "        embedding_data : dataframe, optional\n",
    "            A datafrane with the categorical features (default = None)\n",
    "            \n",
    "        dropout : list or float, optional\n",
    "            If it is float, then creates dropout layers with p=dropout with lenght equal to the lenght of hidden layers\n",
    "            If it is list, a list of float must be given with lenght equal to the lenght of hidden layers\n",
    "            (default = None)\n",
    "            \n",
    "        transformation_list : list, optional\n",
    "            If True, sets as activation function the Sigmoid function \n",
    "            and giving greater weight on the observations with greater target value (default = None)\n",
    "            \n",
    "        l1_weight: int or float, optional\n",
    "        Weight parameter for L1 regularization (default=0)\n",
    "    \n",
    "        l2_weight: int or float, optional\n",
    "            Weight parameter for L2 regularization (default=0)\n",
    "\n",
    "        weights: boolean, optional\n",
    "            If True, each sample is weighted based on the target value (default =  False)\n",
    "        \"\"\"\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "                \n",
    "        if dropout != None and embedding_data is None:\n",
    "            if not isinstance(dropout, float) and len(dropout)!=len(hidden_layers):\n",
    "                raise ValueError('Dropout list and hidden_layers list must be of the same size')\n",
    "                \n",
    "        if dropout != None and embedding_data is not None:\n",
    "            if not isinstance(dropout, float) and len(dropout)!=len(hidden_layers)+1:\n",
    "                raise ValueError('Dropout list must be one element greater than the hidden layers list')\n",
    "                \n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.batchNorm_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        self.embeddings_layers = nn.ModuleList()\n",
    "        self.model_type = model_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.transformation_list = transformation_list\n",
    "        self.weights = weights\n",
    "        self.l1_weight = l1_weight\n",
    "        self.l2_weight = l2_weight\n",
    "        self.early_stop = early_stop\n",
    "        self.embedding_data = embedding_data\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        if self.model_type == 'classification':\n",
    "            self.classification_activation =  nn.Softmax()\n",
    "            \n",
    "        if isinstance(early_stop, tuple):\n",
    "            self.early_stop = EarlyStopping(tolerance=early_stop[0], min_delta=early_stop[1])\n",
    "        \n",
    "        self.hidden_layers = [num_features] + hidden_layers + [num_class]\n",
    "        \n",
    "        if embedding_data is not None:\n",
    "            embedding_sizes = set_embedding_sizes(embedding_data.values)\n",
    "            num_features = num_features - len(embedding_sizes)\n",
    "            self.embeddings_layers = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "            n_emb = sum(e.embedding_dim for e in self.embeddings_layers) #length of all embeddings combined\n",
    "    \n",
    "            # substract from the first layer input the number of categorical  features,\n",
    "            # and add the sum of the output of the embeddings layers\n",
    "            self.hidden_layers[0] = self.hidden_layers[0]-len(embedding_sizes) + n_emb\n",
    "            \n",
    "        if dropout != None:\n",
    "            if isinstance(dropout, float):\n",
    "                if dropout < 0 or dropout > 1:\n",
    "                    raise ValueError('Dropout rate must be in [0,1]')\n",
    "                if embedding_data is not None:\n",
    "                    for i in range(len(hidden_layers)+1):\n",
    "                        self.dropout_layers.append(nn.Dropout(p=dropout))\n",
    "                else:\n",
    "                    for i in range(len(hidden_layers)):\n",
    "                        self.dropout_layers.append(nn.Dropout(p=dropout))\n",
    "            else:       \n",
    "                for i in range(len(dropout)):\n",
    "                    self.dropout_layers.append(nn.Dropout(p=dropout[i]))\n",
    "            \n",
    "\n",
    "        for i in range(len(self.hidden_layers)-1):\n",
    "            self.linear_layers.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i+1]))\n",
    "\n",
    "        for i in self.hidden_layers[1:-1]:\n",
    "            self.batchNorm_layers.append(nn.BatchNorm1d(i))\n",
    "\n",
    "        if 'normalization' in self.transformation_list:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        'Initialization of the weights'\n",
    "        if isinstance(module, nn.Linear):\n",
    "#             torch.nn.init.xavier_normal_(module.weight)\n",
    "            torch.nn.init.kaiming_normal_(module.weight)\n",
    "            module.bias.data.fill_(0)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            module.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x_cont, x_cat=None):\n",
    "        \"\"\"Initialization of the architecture of the neural network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_cont : DataLoader\n",
    "            A DataLoader object of the arithemetic features\n",
    "            \n",
    "        x_cat : DataLoader\n",
    "            A DataLoader object of the categorical features\n",
    "        \"\"\"\n",
    "        dropout_layers = self.dropout_layers\n",
    "        if len(self.embeddings_layers) != 0: \n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings_layers)]\n",
    "            x = torch.cat(x, 1)\n",
    "            if len(self.dropout_layers) != 0:\n",
    "                x = dropout_layers[0](x)\n",
    "                dropout_layers = dropout_layers[1:] \n",
    "            x_cont = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        out = self.linear_layers[0](x_cont)\n",
    "        out = self.batchNorm_layers[0](out)\n",
    "        out = self.activation(out)\n",
    "        if len(self.dropout_layers) != 0:\n",
    "            out = dropout_layers[0](out)\n",
    "\n",
    "        for i in range(1,len(self.hidden_layers)-2):\n",
    "            out = self.linear_layers[i](out)\n",
    "            out = self.batchNorm_layers[i](out)\n",
    "            out = self.activation(out)\n",
    "            if len(self.dropout_layers) != 0:\n",
    "                out = dropout_layers[i](out)\n",
    "\n",
    "        out = self.linear_layers[-1](out)\n",
    "        if self.model_type != 'classification':\n",
    "            out = self.activation(out)\n",
    "        else: \n",
    "            out = self.classification_activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a57a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, train, evals, ylabel):\n",
    "    \"\"\"Prints the plot of evaluation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : list\n",
    "        A list with the epochs\n",
    "        \n",
    "    train : list\n",
    "        A list with the prediction score on the train set\n",
    "        \n",
    "    evals : list\n",
    "        A list with the prediction score on the test set\n",
    "        \n",
    "    y_label : str\n",
    "        The label of the y-axis\n",
    "    \"\"\"\n",
    "    for i in range(len(train)):\n",
    "        if evals[i] > train[i]:\n",
    "            break\n",
    "    plt.plot(epochs, train, label='Train')\n",
    "    plt.plot(epochs, evals, label='Eval')\n",
    "    plt.vlines(x = i+1,ls='--', ymin = 0, ymax = max(max(train),max(evals)), colors = 'grey', label = 'x = '+str(i+1))\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94ca782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test, model_type):\n",
    "    \"\"\"Calculates the MAE of each epoch\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : list\n",
    "        The predictions of the model\n",
    "        \n",
    "    y_test : list\n",
    "        The actual values      \n",
    "        \n",
    "    model_type : str\n",
    "        The type odf the model to be implemented.\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    acc : float\n",
    "        The MAE of the predictions\n",
    "    \"\"\"\n",
    "    if model_type =='classification':\n",
    "        _, y_pred = torch.max(y_pred, dim=1)\n",
    "        _, labels = torch.max(y_test, dim=1)\n",
    "        acc = mean_absolute_error(labels, y_pred)\n",
    "    else:\n",
    "        y_pred = torch.round(y_pred)\n",
    "        acc = mean_absolute_error(y_test, y_pred.detach().numpy())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NN_features(model, training_set, testing_set, X_train_minmax, X_test_minmax, features, delete=False, *args):\n",
    "    \n",
    "    training_generator = torch.utils.data.DataLoader(training_set, batch_size=X_train_minmax.shape[0])\n",
    "    testing_generator = torch.utils.data.DataLoader(testing_set, batch_size=X_test_minmax.shape[0])\n",
    "    batch = next(iter(training_generator))\n",
    "    train, _ = batch\n",
    "    batch = next(iter(testing_generator))\n",
    "    test, _ = batch\n",
    "    \n",
    "    e = shap.DeepExplainer(model, train)\n",
    "    shap_values = e.shap_values(test)\n",
    "    shap.summary_plot(shap_values, features=test, feature_names=features, plot_type='bar')\n",
    "    \n",
    "    mean_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "    if delete:\n",
    "        mean_shap = np.delete(mean_shap, args[0])\n",
    "\n",
    "    weights = mean_shap / mean_shap.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006cad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_set, test_set, learning_rate=None, epochs=None, batch_size=None, \n",
    "             early_stop=None, features=None, max_val=None):\n",
    "    \"\"\" Trainning of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A FeedforwardNeuralNetModel model\n",
    "        \n",
    "    train_set : Dataset\n",
    "        A Dataset object with the train set\n",
    "        \n",
    "    test_set : Dataset\n",
    "        A Dataset object with the test set\n",
    "        \n",
    "    learning_rate : int, optional\n",
    "        The learning_rate of the training process. (default = None)\n",
    "        \n",
    "    epochs : int, optional\n",
    "        The number of epochs for the training. (default = None)\n",
    "        \n",
    "    batch_size : int, optional\n",
    "        The size of each batch in each iteration. (default = None)\n",
    "        \n",
    "    ealry_stop : tuple, optional\n",
    "        Set the (tolerance,min_dleta), the trainning of the model may stop earlier than the epochs defined. (default = None)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    results_train: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the train set\n",
    "        \n",
    "    results_test: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the test set\n",
    "        \n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A trained FeedforwardNeuralNetModel model \n",
    "    \"\"\"    \n",
    "    loss_stats = {\n",
    "    \"train\": [],\n",
    "    \"val\": []\n",
    "    }\n",
    "    accuracy_stats = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = model.epochs\n",
    "    \n",
    "    if learning_rate is None:\n",
    "        learning_rate = model.learning_rate\n",
    "        \n",
    "    if batch_size is None:\n",
    "        batch_size = model.batch_size\n",
    "        \n",
    "    if isinstance(early_stop, tuple):\n",
    "        model.early_stop = EarlyStopping(tolerance=early_stop[0], min_delta=early_stop[1])\n",
    "\n",
    "    \n",
    "    criterion = model.criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if model.weights:\n",
    "        b_val = train_set.y.mean().item()\n",
    "        weights = [sigmoid(train_set.y[i],0.005,b=b_val) for i in range(int(train_set.X.shape[0]))]\n",
    "        weighted_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(train_set.X.shape[0]), replacement=True)\n",
    "        training_generator = torch.utils.data.DataLoader(train_set, batch_size=batch_size,sampler=weighted_sampler)\n",
    "    else:\n",
    "        training_generator = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    \n",
    "    testing_generator = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        if train_set.emb:\n",
    "            for X, X_emb, y in training_generator:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_train_pred = model(X ,X_emb)\n",
    "\n",
    "                train_loss = criterion(y_train_pred, y)\n",
    "                train_acc = multi_acc(y_train_pred, y, model.model_type)\n",
    "                \n",
    "                l1_penalty = model.l1_weight * sum([p.abs().sum() for p in model.parameters()])\n",
    "                l2_penalty = model.l2_weight * sum([(p**2).sum() for p in model.parameters()])\n",
    "                train_loss = train_loss + l1_penalty + l2_penalty\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_epoch_loss += train_loss.item()\n",
    "                train_epoch_acc += train_acc.item()\n",
    "        else:\n",
    "            for X, y in training_generator:\n",
    "                optimizer.zero_grad()\n",
    "                y_train_pred = model(X)\n",
    "                train_loss = criterion(y_train_pred, y)\n",
    "                train_acc = multi_acc(y_train_pred, y, model.model_type)\n",
    "                \n",
    "                l1_penalty = model.l1_weight * sum([p.abs().sum() for p in model.parameters()])\n",
    "                l2_penalty = model.l2_weight * sum([(p**2).sum() for p in model.parameters()])\n",
    "                train_loss = train_loss + l1_penalty + l2_penalty\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_epoch_loss += train_loss.item()\n",
    "                train_epoch_acc += train_acc.item()\n",
    "        \n",
    "\n",
    "        # VALIDATION\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            if test_set.emb:\n",
    "                for X, X_emb, y in testing_generator:\n",
    "\n",
    "                    y_val_pred = model(X, X_emb)\n",
    "\n",
    "                    val_loss = criterion(y_val_pred, y)\n",
    "                    val_acc = multi_acc(y_val_pred, y, model.model_type)\n",
    "\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "                    val_epoch_acc += val_acc.item()\n",
    "                    \n",
    "            else:\n",
    "                for X, y in testing_generator:\n",
    "\n",
    "                    y_val_pred = model(X)\n",
    "\n",
    "                    val_loss = criterion(y_val_pred, y)\n",
    "                    val_acc = multi_acc(y_val_pred, y, model.model_type)\n",
    "\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "                    val_epoch_acc += val_acc.item()\n",
    "                    \n",
    "\n",
    "        loss_stats['train'].append(train_epoch_loss / len(training_generator))\n",
    "        loss_stats['val'].append(val_epoch_loss / len(testing_generator))\n",
    "\n",
    "        accuracy_stats['train'].append(train_epoch_acc / len(training_generator))\n",
    "        accuracy_stats['val'].append(val_epoch_acc / len(testing_generator))\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch+1 + 0:03}: | Train Loss: {train_epoch_loss / len(training_generator):.5f} | Val Loss: {val_epoch_loss / len(testing_generator):.5f} | Train Acc: {train_epoch_acc / len(training_generator):.3f}| Val Acc: {val_epoch_acc / len(testing_generator):.3f}')\n",
    "\n",
    "        if model.early_stop is not None:\n",
    "            model.early_stop(train_epoch_loss / len(training_generator), val_epoch_loss / len(testing_generator))\n",
    "            if model.early_stop.early_stop_flag:\n",
    "                print(\"We are at epoch:\", epoch+1)\n",
    "                break\n",
    "\n",
    "    my_plot(np.linspace(1, epoch+1, epoch+1).astype(int), loss_stats['train'], loss_stats['val'],'MSE Loss')\n",
    "    my_plot(np.linspace(1, epoch+1, epoch+1).astype(int), accuracy_stats['train'], accuracy_stats['val'],'MAE')\n",
    "    \n",
    "    if train_set.emb:\n",
    "        train_predict = model(train_set.X, train_set.X_emb)\n",
    "        test_predict = model(test_set.X, test_set.X_emb)\n",
    "    else:\n",
    "        train_predict = model(train_set.X)\n",
    "        test_predict = model(test_set.X)\n",
    "       \n",
    "    \n",
    "    if model.model_type == 'classification':\n",
    "        _, train_predict = torch.max(train_predict, dim=1)\n",
    "        _, test_predict = torch.max(test_predict, dim=1)\n",
    "        \n",
    "        _, train_y = torch.max(train_set.y, dim=1)\n",
    "        _, test_y = torch.max(test_set.y, dim=1)\n",
    "        \n",
    "        train_y = train_y.tolist()\n",
    "        test_y = test_y.tolist()\n",
    "\n",
    "    train_predict = train_predict.tolist()\n",
    "    test_predict = test_predict.tolist()\n",
    "    \n",
    "    if model.model_type != 'classification':\n",
    "        if 'normalization' in model.transformation_list:\n",
    "            train_y = denormalizeData([e[0].item() for e in train_set.y], max_val=max_val)\n",
    "            train_predict = [round(x) for x in denormalizeData([e[0] for e in train_predict], max_val=max_val)]\n",
    "\n",
    "            test_y = denormalizeData([e[0].item() for e in test_set.y], max_val=max_val)\n",
    "            test_predict = [round(x) for x in denormalizeData([e[0] for e in test_predict], max_val=max_val)]\n",
    "\n",
    "        elif 'log' in model.transformation_list :\n",
    "            train_y = expData([e[0].item() for e in train_set.y])\n",
    "            train_predict = [round(x) for x in expData([e[0] for e in train_predict])]\n",
    "\n",
    "            test_y = expData([e[0].item() for e in test_set.y])\n",
    "            test_predict = [round(x) for x in expData([e[0] for e in test_predict])]\n",
    "            \n",
    "        else:\n",
    "            train_y = [e[0].item() for e in train_set.y]\n",
    "            train_predict = [round(e[0]) for e in train_predict]\n",
    "\n",
    "            test_y = [e[0].item() for e in test_set.y]\n",
    "            test_predict = [round(e[0]) for e in test_predict]\n",
    "\n",
    "        \n",
    "        \n",
    "    results_train = {'actual': train_y, 'prediction': train_predict}\n",
    "    results_train = pd.DataFrame.from_dict(results_train)\n",
    "    \n",
    "    results_test = {'actual': test_y, 'prediction': test_predict}\n",
    "    results_test = pd.DataFrame.from_dict(results_test)\n",
    "    \n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "    results_train.loc[results_train['prediction'] < 0,'prediction'] = 0\n",
    "    results_train.loc[results_train['prediction'] > max_val,'prediction'] = max_val\n",
    "    \n",
    "    if features is not None:\n",
    "        find_NN_features(model, train_set, test_set, train_X, test_X, features)\n",
    "    \n",
    "    return results_train, results_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9dba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_nn(model, train_set, test_set, learning_rate=None, epochs=None, \n",
    "               batch_size=None, early_stop=None, features=None, max_val=None):\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = model.epochs\n",
    "    \n",
    "    if learning_rate is None:\n",
    "        learning_rate = model.learning_rate\n",
    "        \n",
    "    if batch_size is None:\n",
    "        batch_size = model.batch_size\n",
    "        \n",
    "    if isinstance(early_stop, tuple):\n",
    "        early_stop = EarlyStopping(tolerance=early_stop[0], min_delta=early_stop[1])\n",
    "    else:\n",
    "        early_stop = model.early_stop  \n",
    "    \n",
    "    criterion = model.criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if model.weights:\n",
    "        weights = [sigmoid(train_set.y[i],0.005,1) for i in range(int(train_set.X.shape[0]))]\n",
    "        weighted_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(train_set.X.shape[0]), replacement=True)\n",
    "        training_generator = torch.utils.data.DataLoader(train_set, batch_size=batch_size,sampler=weighted_sampler)\n",
    "    else:\n",
    "        training_generator = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    \n",
    "    testing_generator = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        if train_set.emb:\n",
    "            for X, X_emb, y in training_generator:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_train_pred = model(X ,X_emb)\n",
    "\n",
    "                train_loss = criterion(y_train_pred, y)\n",
    "                train_acc = multi_acc(y_train_pred, y, model.model_type)\n",
    "                \n",
    "                l1_penalty = model.l1_weight * sum([p.abs().sum() for p in model.parameters()])\n",
    "                l2_penalty = model.l2_weight * sum([(p**2).sum() for p in model.parameters()])\n",
    "                train_loss = train_loss + l1_penalty + l2_penalty\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_epoch_loss += train_loss.item()\n",
    "                train_epoch_acc += train_acc.item()\n",
    "        else:\n",
    "            for X, y in training_generator:\n",
    "                optimizer.zero_grad()\n",
    "                y_train_pred = model(X)\n",
    "                train_loss = criterion(y_train_pred, y)\n",
    "                train_acc = multi_acc(y_train_pred, y, model.model_type)\n",
    "                \n",
    "                l1_penalty = model.l1_weight * sum([p.abs().sum() for p in model.parameters()])\n",
    "                l2_penalty = model.l2_weight * sum([(p**2).sum() for p in model.parameters()])\n",
    "                train_loss = train_loss + l1_penalty + l2_penalty\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_epoch_loss += train_loss.item()\n",
    "                train_epoch_acc += train_acc.item()\n",
    "\n",
    "        if early_stop:\n",
    "            early_stopping(train_epoch_loss / len(training_generator), val_epoch_loss / len(testing_generator))\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Stopped at epoch:\", epoch+1)\n",
    "                break\n",
    "    \n",
    "    if train_set.emb:\n",
    "        test_predict = model(test_set.X, test_set.X_emb)\n",
    "    else:\n",
    "        test_predict = model(test_set.X)\n",
    "       \n",
    "    \n",
    "    if model.model_type == 'classification':\n",
    "        _, test_predict = torch.max(test_predict, dim=1)\n",
    "        _, test_y = torch.max(test_set.y, dim=1)\n",
    "        test_y = test_y.tolist()\n",
    "\n",
    "    test_predict = test_predict.tolist()\n",
    "    \n",
    "    if model.model_type != 'classification':\n",
    "        if 'normalization' in model.transformation_list:\n",
    "            test_y = denormalizeData([e[0].item() for e in test_set.y], max_val=max_val)\n",
    "            test_predict = [round(x) for x in denormalizeData([e[0] for e in test_predict], max_val=max_val)]\n",
    "\n",
    "        elif 'log' in model.transformation_list :\n",
    "            test_y = expData([e[0].item() for e in test_set.y])\n",
    "            test_predict = [round(x) for x in expData([e[0] for e in test_predict])]\n",
    "        else:\n",
    "            test_y = [e[0].item() for e in test_set.y]\n",
    "            test_predict = [round(e[0]) for e in test_predict]\n",
    "            \n",
    "    results_test = {'actual': test_y, 'prediction': test_predict}\n",
    "    results_test = pd.DataFrame.from_dict(results_test)\n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "    \n",
    "    if features is not None:\n",
    "        find_NN_features(model, train_set, test_set, train_X, test_X, features)\n",
    "    \n",
    "    return results_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
