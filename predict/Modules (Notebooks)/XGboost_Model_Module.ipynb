{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d653348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1b7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(y, min_val=0, max_val=9):\n",
    "    'Transforms the target data to 0-1 range in order to use the sigmoid function as activation'\n",
    "    return pd.Series((np.array(y) - min_val) / (max_val-min_val))\n",
    "\n",
    "def logData(y):\n",
    "    'Transforms the target variable to log(target) in order to follow a more normal disribution'\n",
    "    return np.log1p(y)\n",
    "\n",
    "def expData(y):\n",
    "    'Calculate the exponential value of the target variable'\n",
    "    return np.expm1(y)\n",
    "\n",
    "def denormalizeData(y, min_val=0, max_val=9):\n",
    "    'Transforms the data from 0-1 range to the initial 0-9 range'\n",
    "    return pd.Series((np.array(y)*(max_val-min_val))+min_val)\n",
    "\n",
    "def sigmoid(x,a=1,b=0):\n",
    "    return 1.0 / (1.0 + np.exp(a*(-x+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4844c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations_xgboost(data, model_type, test=None, evaluation=False, transformation_list=['scaling'],\n",
    "                    embedding_data=None):\n",
    "    \"\"\"Executes scaling, augmentation or label endconding on the dataset \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        A dataframe containing all the data or the data of trainning.\n",
    "        If test not given, then data will be randomnly slpit in 80-20% train and test set\n",
    "        \n",
    "    model_type : str\n",
    "        The type of the model to be implemented.\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    test : dataframe, optional\n",
    "        A dataframe containing all the data for testing (default = None)\n",
    "        \n",
    "    scaling : boolean, optional\n",
    "        If True, perofrms scaling on numerical features (default = False)\n",
    "        \n",
    "    augment : boolean, optional\n",
    "        If True, augments the train data with existing observations\n",
    "        and giving greater weight on the observations with greater target value (default = False)\n",
    "        \n",
    "    embedding_data : dataframe, optional\n",
    "        A datafrane with the categorical features (default = None)\n",
    "        \n",
    "    evaluation : boolean, optional\n",
    "        If True, 20% of the observations of train set will be held for evaluation set (default = False)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    train_X: numpy array\n",
    "        A numpy array with independent variables for training\n",
    "        \n",
    "    train_y: pd.Series\n",
    "        A array with the dependent variables (target) for training\n",
    "        \n",
    "    test_X: numpy array\n",
    "        A numpy array with independent variables for test\n",
    "        \n",
    "    test_y: pd.Series\n",
    "        A array with the dependent variables (target) for test\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if test is None:    \n",
    "        X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        train_X,test_X,train_y,test_y = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    else:\n",
    "        data = data.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "        train_X, train_y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        test_X, test_y = test.iloc[:,:-1], test.iloc[:,-1]\n",
    "        \n",
    "    train_X = train_X.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    test_X = test_X.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "        \n",
    "    if evaluation:\n",
    "        train_X,eval_X,train_y,eval_y = train_test_split(train_X, train_y, test_size=0.20, random_state=1)\n",
    "        eval_X = eval_X.reset_index(drop=True)\n",
    "        eval_y = eval_y.reset_index(drop=True)\n",
    "        \n",
    "    if model_type == 'mosquito_regression':       \n",
    "        percentile = round(np.percentile(train_y, 95))\n",
    "        train_y.loc[train_y >= percentile] = percentile\n",
    "\n",
    "    if 'augmentation' in transformation_list:\n",
    "        augment_index = train_y.sample(frac=0.4, weights=train_y, random_state=1, replace=True).index\n",
    "        train_X = pd.concat([train_X,train_X.iloc[augment_index,:]]).reset_index(drop=True)\n",
    "        train_y = pd.concat([train_y,train_y[augment_index]]).reset_index(drop=True)\n",
    "    \n",
    "    if embedding_data is not None:\n",
    "        embeddings = embedding_data.columns.tolist()\n",
    "        \n",
    "        embedded_columns_train = train_X[embeddings] #categorical columns\n",
    "        train_X = train_X.drop(columns=embeddings)\n",
    "\n",
    "        embedded_columns_test = test_X[embeddings] #categorical columns\n",
    "        test_X = test_X.drop(columns=embeddings)           \n",
    "            \n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        encoder.fit(embedded_columns_train)\n",
    "        if evaluation:\n",
    "            embedded_columns_eval = eval_X[embeddings] #categorical columns\n",
    "            eval_X = eval_X.drop(columns=embeddings)\n",
    "            embedded_columns_eval= encoder.transform(embedded_columns_eval)\n",
    "        embedded_columns_train = encoder.transform(embedded_columns_train)\n",
    "        embedded_columns_test = encoder.transform(embedded_columns_test)\n",
    "\n",
    "        train_X_emb = pd.DataFrame(embedded_columns_train,columns=encoder.get_feature_names_out().tolist())  \n",
    "        test_X_emb = pd.DataFrame(embedded_columns_test,columns=encoder.get_feature_names_out().tolist())\n",
    "        if evaluation:\n",
    "            eval_X_emb = pd.DataFrame(embedded_columns_eval,columns=encoder.get_feature_names_out().tolist())\n",
    "            \n",
    "    if 'scaling' in transformation_list:\n",
    "        scaler = StandardScaler()\n",
    "        cols = train_X.columns.tolist()\n",
    "        train_X = scaler.fit_transform(train_X)\n",
    "        train_X = pd.DataFrame(train_X, columns=cols, dtype='category')\n",
    "        test_X = scaler.transform(test_X)\n",
    "        test_X = pd.DataFrame(test_X, columns=cols, dtype='category')\n",
    "        if evaluation:\n",
    "            eval_X = scaler.transform(eval_X)\n",
    "            eval_X = pd.DataFrame(eval_X, columns=cols, dtype='category')\n",
    "            \n",
    "    if 'normalization' in transformation_list:\n",
    "        min_val = train_y.min()\n",
    "        max_val = train_y.max()\n",
    "        test_y = normalizeData(test_y, min_val, max_val)\n",
    "        train_y = normalizeData(train_y, min_val, max_val)\n",
    "        if evaluation:\n",
    "            eval_y = normalizeData(eval_y, min_val, max_val)\n",
    "            \n",
    "    if 'log' in transformation_list:\n",
    "        test_y = logData(test_y)\n",
    "        train_y = logData(train_y)\n",
    "        if evaluation:\n",
    "            eval_y = logData(eval_y)\n",
    "            \n",
    "    if embedding_data is not None:\n",
    "        train_X = pd.concat([train_X,train_X_emb],axis=1)\n",
    "        test_X = pd.concat([test_X, test_X_emb],axis=1)\n",
    "        if evaluation:\n",
    "            eval_X = pd.concat([eval_X,eval_X_emb],axis=1)\n",
    "            \n",
    "    if evaluation:\n",
    "        return train_X, train_y, eval_X, eval_y, test_X, test_y\n",
    "    else:       \n",
    "        return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973ae936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgboost_model:\n",
    "    def __init__(self, model_type, learning_rate=0.3,\n",
    "                 embedding_data=None,  transformation_list=[],\n",
    "                 early_stop = False, l1_weight=0, l2_weight=0, weights=False):\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embedding_data = embedding_data\n",
    "        self.transformation_list = transformation_list\n",
    "        self.early_stop = early_stop\n",
    "        self.weights = weights\n",
    "        self.estimators = None\n",
    "        self.depth = None\n",
    "        self.l1_weight = l1_weight\n",
    "        self.l2_weight = l2_weight\n",
    "        self.features = None\n",
    "    \n",
    "        if self.model_type == 'classification':\n",
    "            self.model = xgb.XGBClassifier()\n",
    "        else:\n",
    "            self.model = xgb.XGBRegressor()\n",
    "            \n",
    "        xgb_params = {'learning_rate': self.learning_rate,\n",
    "                      'random_state':1,\n",
    "                      'lambda':self.l2_weight,\n",
    "                      'alpha':self.l1_weight}\n",
    "        self.model.set_params(**xgb_params)\n",
    "        \n",
    "    def tune_parameters(self, train_X, train_y, flag = False):\n",
    "        \"\"\"Returns the depth and the number of estimators with the minimum MAE\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Dataframe\n",
    "            A Daframe containing the data\n",
    "\n",
    "        flag : boolean, otional\n",
    "            If true plots are printed (default=False)\n",
    "\n",
    "        transform : boolean, optional\n",
    "            If True, performs log transformation of the target variable (default = False)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        depth : int\n",
    "            The max depth of the trees to construct\n",
    "\n",
    "        est : int\n",
    "            The number of trees to construct\n",
    "\n",
    "        \"\"\"\n",
    "        train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.20, random_state=1)\n",
    "        \n",
    "        tr_depth = []\n",
    "        val_depth = []\n",
    "        tr_est = []\n",
    "        val_est = []\n",
    "\n",
    "        for i in range(1,11):\n",
    "            xgb_params = {'n_estimators': 5,\n",
    "                          'max_depth': i}\n",
    "            self.model.set_params(**xgb_params)\n",
    "            self.model.fit(train_X, train_y)               \n",
    "            predictions_train = self.model.predict(train_X)\n",
    "            predictions_val = self.model.predict(val_X)\n",
    "    #         if 'log' in self.transform_list :\n",
    "    #             predictions_train = expData(predictions_train)\n",
    "    #             predictions_val = expData(predictions_val)\n",
    "    #         if 'normalization' in self.transform_list :\n",
    "    #             predictions_train = denormalizeData(predictions_train,max_val=train_y.max())\n",
    "    #             predictions_val = denormalizeData(predictions_val,max_val=train_y.max())\n",
    "    #         predictions_train = np.round(predictions_train)\n",
    "    #         predictions_val = np.round(predictions_val)\n",
    "            mae_train = mean_absolute_error(train_y, predictions_train)\n",
    "            mae_val = mean_absolute_error(val_y, predictions_val)\n",
    "            tr_depth.append(mae_train)\n",
    "            val_depth.append(mae_val)\n",
    "        self.depth = val_depth.index(min(val_depth)) + 1\n",
    "\n",
    "\n",
    "        for i in range(1,36):\n",
    "            xgb_params = {'n_estimators': i,\n",
    "                          'max_depth': self.depth}\n",
    "            self.model.set_params(**xgb_params)\n",
    "            self.model.fit(train_X, train_y)                \n",
    "            predictions_train = self.model.predict(train_X)\n",
    "            predictions_val = self.model.predict(val_X)\n",
    "    #         if 'log' in self.transform_list :\n",
    "    #             predictions_train = expData(predictions_train)\n",
    "    #             predictions_val = expData(predictions_val)\n",
    "    #         if 'normalization' in self.transform_list :\n",
    "    #             predictions_train = denormalizeData(predictions_train,max_val=train_y.max())\n",
    "    #             predictions_val = denormalizeData(predictions_val,max_val=train_y.max())\n",
    "    #         predictions_train = np.round(predictions_train)\n",
    "    #         predictions_val = np.round(predictions_val)\n",
    "            mae_train = mean_absolute_error(train_y, predictions_train)\n",
    "            mae_val = mean_absolute_error(val_y, predictions_val)\n",
    "            tr_est.append(mae_train)\n",
    "            val_est.append(mae_val)\n",
    "        self.estimators = val_est.index(min(val_est)) + 1\n",
    "\n",
    "        if flag:\n",
    "            labels = list(range(1,len(tr_depth)+1))\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.grid()\n",
    "            plt.plot(labels,tr_depth,label='train')\n",
    "            plt.plot(labels,val_depth,label='validation')\n",
    "            plt.vlines(x = self.depth,ls='--', ymin = 0, ymax = max(max(tr_depth),max(val_depth)),\n",
    "                       colors = 'grey', label = 'x = '+str(self.depth))\n",
    "            ax.set_xticks(np.arange(len(tr_depth)+1))\n",
    "            plt.legend()\n",
    "            plt.xlabel('max_depth')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.show()\n",
    "\n",
    "            labels = list(range(1,len(tr_est)+1))\n",
    "            fig, ax = plt.subplots(figsize=(10,6))\n",
    "            plt.grid()\n",
    "            plt.plot(labels,tr_est,label='train')\n",
    "            plt.plot(labels,val_est,label='validation')\n",
    "            plt.vlines(x = self.estimators,ls='--', ymin = 0, ymax = max(max(tr_est),max(val_est)),\n",
    "                       colors = 'grey', label = 'x = '+str(self.estimators))\n",
    "            ax.set_xticks(np.arange(len(tr_est)+1))\n",
    "            plt.legend()\n",
    "            plt.xlabel('n_estimators')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.show()\n",
    "\n",
    "        return self.depth, self.estimators\n",
    "    \n",
    "    def select_features(self, train_X, train_y, grid=False):\n",
    "        \"\"\"Selects which features to use in the training process.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        df : dataframe\n",
    "            A dataframe containing the data\n",
    "\n",
    "        depth : int\n",
    "            The dpeth of the trees to construct\n",
    "\n",
    "        estimators : int\n",
    "            The number of trees to construct\n",
    "\n",
    "        grid : boolean, optional\n",
    "            If True it creates a grid of points around the (depth, estimator) in search of a better combination.\n",
    "            However it highly increases time complexity (default=False)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        selected_features : lst\n",
    "            A list containing the features selected\n",
    "\n",
    "        depth : int\n",
    "            The depth of the trees to construct\n",
    "\n",
    "        estimators : int\n",
    "            The number of trees to constuct\n",
    "        \"\"\"   \n",
    "        if grid:\n",
    "            r2=-1\n",
    "            final = None\n",
    "            dep=0\n",
    "            est = 0\n",
    "            for i in range(self.depth-2,self.depth+3):\n",
    "                for j in range(self.estimators-2,self.estimators+3):\n",
    "                    xgb_params = {'n_estimators': i,\n",
    "                                  'max_depth': j}\n",
    "                    self.model.set_params(**xgb_params)\n",
    "                    rfe = RFECV(self.model, step=1, cv=10,scoring = 'neg_mean_absolute_error')\n",
    "                    X_train_rfe = rfe.fit_transform(train_X,train_y)\n",
    "                    if rfe.score(X_train,y_train) > r2:\n",
    "                        final = rfe\n",
    "                        dep = i\n",
    "                        est = j\n",
    "                        r2 = rfe.score(X_train,y_train)\n",
    "            rfe = final\n",
    "            self.depth = dep\n",
    "            self.estimators = est\n",
    "        else:\n",
    "            xgb_params = {'n_estimators': self.estimators,\n",
    "                          'max_depth': self.depth}\n",
    "            self.model.set_params(**xgb_params)\n",
    "            rfe = RFECV(self.model, step=1, cv=10,scoring = 'neg_mean_absolute_error')\n",
    "            X_train_rfe = rfe.fit_transform(train_X,train_y)\n",
    "\n",
    "        #print the features selected\n",
    "        cols = list(train_X.columns) \n",
    "        temp = pd.Series(rfe.support_,index = cols)\n",
    "        self.features = temp[temp==True].index\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of features selected\")\n",
    "        plt.ylabel(\"neg mean squared error\")\n",
    "        plt.plot(range(1,len(rfe.cv_results_['mean_test_score']) + 1), rfe.cv_results_['mean_test_score'])\n",
    "        plt.vlines(x = np.argmax(rfe.cv_results_['mean_test_score'])+1,\n",
    "                   ls='--', ymin = np.min(rfe.cv_results_['mean_test_score']),\n",
    "                   ymax = np.max(rfe.cv_results_['mean_test_score']), colors = 'grey',\n",
    "                   label = 'x = '+str(np.argmax(rfe.cv_results_['mean_test_score'])+1))\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print('max_depth: ', self.depth)\n",
    "        print('Number of estimators: ', self.estimators)\n",
    "        print('Selected_features: ', self.features)\n",
    "\n",
    "        return self.features, self.depth, self.estimators\n",
    "    \n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"Prints the plot of feature importance of the model and creates a .csv file with the frature importance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        importance : list\n",
    "            A list containing the importnce of each feature\n",
    "\n",
    "        names : list\n",
    "            A list containing the names of the features\n",
    "\n",
    "        filepath : srt\n",
    "            The path of the file to export a csv with the importance of the featured\n",
    "\n",
    "        case: str, optional\n",
    "            Title of the plot (Area and mosquito genus) (default= '')\n",
    "\n",
    "        export: boolean, optional\n",
    "            Exports a csv with the importance of each figure (default = False)\n",
    "        \"\"\"\n",
    "        #Create arrays from feature importance and feature names\n",
    "        feature_importance = np.array(self.model.feature_importances_)\n",
    "        feature_names = np.array(self.features)\n",
    "\n",
    "        #Create a DataFrame using a Dictionary\n",
    "        data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "        fi_df = pd.DataFrame(data)\n",
    "\n",
    "        #Sort the DataFrame in order decreasing feature importance\n",
    "        fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "        #Define size of bar plot\n",
    "        plt.figure(figsize=(10,8))\n",
    "\n",
    "        #Plot Searborn bar chart\n",
    "        sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "\n",
    "        #Add chart labels\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature Names')\n",
    "        plt.show()\n",
    "        \n",
    "        indicies = ['ndvi', 'ndmi', 'ndwi', 'ndbi',\n",
    "            'ndvi_mean', 'ndmi_mean', 'ndwi_mean', 'ndbi_mean',\n",
    "            'ndvi_std','ndmi_std', 'ndwi_std', 'ndbi_std',]\n",
    "\n",
    "        weather = ['lst', 'lst_day', 'lst_night',\n",
    "                   'lst_jan_day_mean', 'lst_jan_night_mean',\n",
    "                   'lst_feb_day_mean', 'lst_feb_night_mean',\n",
    "                   'lst_mar_day_mean', 'lst_mar_night_mean',\n",
    "                   'lst_apr_day_mean', 'lst_apr_night_mean',\n",
    "                   'acc_rainfall_1week', 'acc_rainfall_2week', 'acc_rainfall_jan',]\n",
    "\n",
    "        geomorphological = ['DISTANCE_TO_COAST', 'DISTANCE_TO_RIVER', 'SLOPE_mean_1km', 'ASPECT_mean_200m',\n",
    "                            'ELEVATION_mean_1km', 'HILLSHADE_mean_1km', 'FS_AREA_1km', 'FLOW_ACCU_200m', 'landcover']\n",
    "\n",
    "        spatiotemporal = ['x', 'y', 'year''null_island_distance', 'vert_distance',\n",
    "                          'days_distance', 'mo_sin', 'mo_cos', 'year',  'summer_days_month']\n",
    "\n",
    "        entomological = ['mosq_sum_month', 'mosq_sum_month_previous_year', 'mosq_sum_year',\n",
    "                         'mosq_sum_previous_2weeks', 'previous_mosq_measure',]\n",
    "\n",
    "        df_indicies = fi_df.loc[fi_df['feature_names'].isin(indicies)]\n",
    "        df_weather = fi_df.loc[fi_df['feature_names'].isin(weather)]\n",
    "        df_geomorphological = fi_df.loc[fi_df['feature_names'].isin(geomorphological)]\n",
    "        df_spatiotemporal = fi_df.loc[fi_df['feature_names'].isin(spatiotemporal)]\n",
    "        df_entomological = fi_df.loc[fi_df['feature_names'].isin(entomological)]\n",
    "\n",
    "        categories = {'Category': ['rs_indicies', 'weather', 'geomorphological', 'spatiotemporal', 'entomological'],\n",
    "\n",
    "                      'Mean_fi': [df_indicies.feature_importance.sum(),\n",
    "                                  df_weather.feature_importance.sum(),\n",
    "                                  df_geomorphological.feature_importance.sum(),\n",
    "                                  df_spatiotemporal.feature_importance.sum(),\n",
    "                                  df_entomological.feature_importance.sum()]}\n",
    "\n",
    "        categories_df = pd.DataFrame.from_dict(categories)\n",
    "\n",
    "        categories_df = categories_df.loc[(categories_df!=0).all(axis=1)].reset_index(drop=True)\n",
    "        categories_df = categories_df.dropna().reset_index(drop=True)\n",
    "\n",
    "        categories_df.sort_values(by = 'Mean_fi', ascending=False)\n",
    "\n",
    "\n",
    "        #Sort the DataFrame in order decreasing feature importance\n",
    "        categories_df.sort_values(by=['Mean_fi'], ascending=False,inplace=True)\n",
    "\n",
    "        #Define size of bar plot\n",
    "        plt.figure(figsize=(10,8))\n",
    "\n",
    "        #Plot Searborn bar chart\n",
    "        sns.barplot(x=categories_df['Mean_fi'], y=categories_df['Category'])\n",
    "\n",
    "        #Add chart labels\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature Category')\n",
    "        plt.show()\n",
    "        \n",
    "        categories = {'Category': ['rs_indicies', 'weather', 'geomorphological', 'spatiotemporal', 'entomological'],\n",
    "\n",
    "                      'Mean_fi': [df_indicies.feature_importance.mean(),\n",
    "                                  df_weather.feature_importance.mean(),\n",
    "                                  df_geomorphological.feature_importance.mean(),\n",
    "                                  df_spatiotemporal.feature_importance.mean(),\n",
    "                                  df_entomological.feature_importance.mean()]}\n",
    "\n",
    "        categories_df = pd.DataFrame.from_dict(categories)\n",
    "\n",
    "        categories_df = categories_df.loc[(categories_df!=0).all(axis=1)].reset_index(drop=True)\n",
    "        categories_df = categories_df.dropna().reset_index(drop=True)\n",
    "\n",
    "        categories_df.sort_values(by = 'Mean_fi', ascending=False)\n",
    "\n",
    "\n",
    "        #Sort the DataFrame in order decreasing feature importance\n",
    "        categories_df.sort_values(by=['Mean_fi'], ascending=False,inplace=True)\n",
    "\n",
    "        #Define size of bar plot\n",
    "        plt.figure(figsize=(10,8))\n",
    "\n",
    "        #Plot Searborn bar chart\n",
    "        sns.barplot(x=categories_df['Mean_fi'], y=categories_df['Category'])\n",
    "\n",
    "        #Add chart labels\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature Category')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006cad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(xg_model, train_X, train_y, test_X, test_y, max_val, fi=True):\n",
    "    \"\"\" Trainning of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A FeedforwardNeuralNetModel model\n",
    "        \n",
    "    train_set : Dataset\n",
    "        A Dataset object with the train set\n",
    "        \n",
    "    test_set : Dataset\n",
    "        A Dataset object with the test set\n",
    "        \n",
    "    learning_rate : int, optional\n",
    "        The learning_rate of the training process. (default = None)\n",
    "        \n",
    "    epochs : int, optional\n",
    "        The number of epochs for the training. (default = None)\n",
    "        \n",
    "    batch_size : int, optional\n",
    "        The size of each batch in each iteration. (default = None)\n",
    "        \n",
    "    ealry_stop : tuple, optional\n",
    "        Set the (tolerance,min_dleta), the trainning of the model may stop earlier than the epochs defined. (default = None)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    results_train: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the train set\n",
    "        \n",
    "    results_test: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the test set\n",
    "        \n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A trained FeedforwardNeuralNetModel model \n",
    "    \"\"\"\n",
    "    weights=None\n",
    "    if xg_model.weights:\n",
    "        b_val = train_y.mean().item()\n",
    "        weights = [sigmoid(train_y[i],0.005,b=b_val) for i in range(int(train_X.shape[0]))]\n",
    "    \n",
    "    if xg_model.features is None:\n",
    "        xg_model.tune_parameters(train_X, train_y, flag=True)\n",
    "        xg_model.select_features(train_X, train_y, grid=False)\n",
    "    \n",
    "    train_X = train_X[xg_model.features]\n",
    "    test_X = test_X[xg_model.features]\n",
    "\n",
    "    xgb_params = {'n_estimators': xg_model.estimators,\n",
    "                  'max_depth': xg_model.depth}\n",
    "    xg_model.model.set_params(**xgb_params)\n",
    "#     eval_set = [(test_X, test_y)]\n",
    "    xg_model.model.fit(train_X, train_y, sample_weight = weights)\n",
    "#                        ,early_stopping_rounds=10, eval_metric=\"mae\", eval_set=eval_set)    \n",
    "    predictions = xg_model.model.predict(test_X)\n",
    "    predictions_train = xg_model.model.predict(train_X)\n",
    "    \n",
    "    if 'normalization' in xg_model.transformation_list:\n",
    "        predictions = denormalizeData(predictions, max_val=max_val)\n",
    "        predictions_train = denormalizeData(predictions_train, max_val=max_val)\n",
    "        train_y = denormalizeData(train_y, max_val=max_val)\n",
    "        test_y = denormalizeData(test_y, max_val=max_val)\n",
    "        \n",
    "    if 'log' in xg_model.transformation_list:\n",
    "        predictions = expData(predictions)\n",
    "        predictions_train = expData(predictions_train)\n",
    "        train_y = expData(train_y)\n",
    "        test_y = expData(test_y)\n",
    "        \n",
    "    predictions =  np.round(predictions)  \n",
    "    predictions_train = np.round(predictions_train)\n",
    "        \n",
    "    results_train = {'actual': train_y, 'prediction': predictions_train}\n",
    "    results_train = pd.DataFrame.from_dict(results_train)\n",
    "    \n",
    "    results_test = {'actual': test_y, 'prediction': predictions}\n",
    "    results_test = pd.DataFrame.from_dict(results_test)\n",
    "    \n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "    results_train.loc[results_train['prediction'] < 0,'prediction'] = 0\n",
    "    results_train.loc[results_train['prediction'] > max_val,'prediction'] = max_val\n",
    "    \n",
    "    if fi:\n",
    "        xg_model.plot_feature_importance()\n",
    "    \n",
    "    return results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xgboost(xg_model, train_X, train_y, test_X, test_y, max_val, fi=True):\n",
    "    \"\"\" Trainning of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A FeedforwardNeuralNetModel model\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    results_train: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the train set\n",
    "        \n",
    "    results_test: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the test set\n",
    "        \n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A trained FeedforwardNeuralNetModel model \n",
    "    \"\"\"\n",
    "    \n",
    "    weights=None\n",
    "    if xg_model.weights:\n",
    "        b_val = train_y.mean().item()\n",
    "        weights = [sigmoid(train_y[i],0.005,b=b_val) for i in range(int(train_X.shape[0]))]\n",
    "        \n",
    "    if xg_model.features is None:\n",
    "        xg_model.tune_parameters(train_X, train_y, flag=True)\n",
    "        xg_model.select_features(train_X, train_y, grid=False)\n",
    "    \n",
    "    train_X = train_X[xg_model.features]\n",
    "    test_X = test_X[xg_model.features]\n",
    "\n",
    "    xg_model.model.fit(train_X, train_y, sample_weight = weights)\n",
    "    predictions = xg_model.model.predict(test_X)\n",
    "    \n",
    "    if 'normalization' in xg_model.transformation_list:\n",
    "        predictions = denormalizeData(predictions, max_val=max_val)\n",
    "\n",
    "    if 'log' in xg_model.transformation_list:\n",
    "        predictions = expData(predictions)\n",
    "\n",
    "    predictions =  np.round(predictions)  \n",
    "\n",
    "    results_test = {'prediction': predictions}\n",
    "    results_test = pd.DataFrame.from_dict(results_test)\n",
    "    \n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "    \n",
    "    if fi:\n",
    "        xg_model.plot_feature_importance()\n",
    "   \n",
    "    return results_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
