{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script preprocess the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import BallTree\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, date_col = 'dt_placement', long_col='x', lat_col='y'):\n",
    "    \"\"\"Reads the data out of an input file (.csv or .xls) \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The path of the file\n",
    "        \n",
    "    date_col : str, , optional\n",
    "        The name of the column with the date (default = 'dt_placement')\n",
    "        \n",
    "    long_col : str, , optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_col : str, , optional\n",
    "        The name of the column with the longitude (default = 'y')\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    data: dataframe\n",
    "        A dataframe created by the input file\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "    If the input file is not .csv or .xls\n",
    "    \n",
    "    KeyError\n",
    "    If there is no column with 'date_col', 'long_col' or 'lat_col' name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # reading the file by xlrd (pip install xlrd)\n",
    "        data = pd.read_excel(filepath)\n",
    "        data = data.replace('<Null>',np.NaN)\n",
    "    except:\n",
    "        try:\n",
    "            # reading as CSV file\n",
    "            data = pd.read_csv(filepath)\n",
    "            data = data.replace('<Null>',np.NaN)\n",
    "        except: \n",
    "            raise NotImplementedError(\"Sorry, give me a .csv or .xls file\")\n",
    "    \n",
    "    try:            \n",
    "        data[date_col] = pd.to_datetime(data[date_col], format=\"%Y-%m-%d\")\n",
    "        data[long_col] = round(data[long_col], 6)\n",
    "        data[lat_col] = round(data[lat_col], 6)\n",
    "    except: \n",
    "        raise KeyError(\"No date, longitude or latitude column with this name was found\")\n",
    "    data.columns = data.columns.str.replace(r'_new', '')\n",
    "    print(data.columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_data(new_path, old, date_col='dt_placement', long_col='x', lat_col='y', exp_path=None):\n",
    "    \"\"\"Merges the new monthly data with the historical ones.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    new_path : str\n",
    "        The file path to the new monthly data\n",
    "        \n",
    "    old : dataframe\n",
    "        A dataframe of the historical data\n",
    "        \n",
    "    date_col : str, , optional\n",
    "        The name of the column with the date (default = 'dt_placement')\n",
    "        \n",
    "    long_col : str, , optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_col : str, , optional\n",
    "        The name of the column with the longitude (default = 'y')\n",
    "        \n",
    "    \"\"\"\n",
    "    new = pd.read_csv(new_path)\n",
    "    new.loc[:,long_col] = round(new.loc[:,long_col], 6)\n",
    "    new.loc[:,lat_col] = round(new.loc[:,lat_col], 6)\n",
    "    new.columns = new.columns.str.replace(r'_new', '')\n",
    "    new[date_col] = pd.to_datetime(new[date_col], format=\"%Y-%m-%d\")\n",
    "    old = pd.concat([old,new],axis=0).reset_index(drop=True)\n",
    "    if exp_path!=None:\n",
    "        old.to_csv(exp_path,index=False)\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_topological(data, filepath, long_column='x', lat_column='y', neighbors=1):\n",
    "    \"\"\"Adds the topological features of each observation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "        A dataframe containing all the EO data\n",
    "    \n",
    "    filepath : str\n",
    "        The path of the file with the topological info\n",
    "        \n",
    "    long_column : str, , optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_column : str, , optional\n",
    "        The name of the column with the latitude (default = 'y')\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    data: dataframe\n",
    "        A dataframe containing the topological info for each observation\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "    If the filepath is not valid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        topological = pd.read_csv(filepath)\n",
    "    except: \n",
    "        raise KeyError(\"Sorry, give me a .csv valid file path.\")\n",
    "    topological[long_column] = round(topological[long_column], 6)\n",
    "    topological[lat_column] = round(topological[lat_column], 6)\n",
    "    topological['x_rad'] = topological[long_column].apply(lambda x: np.deg2rad(x))\n",
    "    topological['y_rad'] = topological[lat_column].apply(lambda x: np.deg2rad(x))\n",
    "    data['x_rad'] = data[long_column].apply(lambda x: np.deg2rad(x))\n",
    "    data['y_rad'] = data[lat_column].apply(lambda x: np.deg2rad(x))\n",
    "    ball = BallTree(topological[[\"y_rad\", \"x_rad\"]].values, metric='haversine')\n",
    "    distances, indices = ball.query(data[[\"y_rad\", \"x_rad\"]].values, k = 1)\n",
    "    distances = [(d * 6371).tolist()[0] for d in distances]\n",
    "    indices = indices.tolist()\n",
    "    indices = [i[0] for i in indices]\n",
    "    del data['x_rad']\n",
    "    del data['y_rad']\n",
    "    del topological['x_rad']\n",
    "    del topological['y_rad']\n",
    "    del topological[long_column]\n",
    "    del topological[lat_column]\n",
    "    data['neighbors'] = indices\n",
    "    data = pd.merge(data, topological, how='left',left_on = [data.neighbors], right_index=True)\n",
    "    del data['neighbors']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_landcover(data, filepath, long_column='x', lat_column='y', date_column='dt_placement', neighbors=1):\n",
    "    \"\"\"Adds the landcover features of each observation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "        A dataframe containing all the EO data\n",
    "    \n",
    "    filepath : str\n",
    "        The path of the file with the landcover info\n",
    "        \n",
    "    long_column : str, , optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_column : str, , optional\n",
    "        The name of the column with the latitude (default = 'y')\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    data: dataframe\n",
    "        A dataframe containing the landcover info for each observation\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "    If the filepath is not valid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        landcover = pd.read_csv(filepath)\n",
    "    except: \n",
    "        raise KeyError(\"Sorry, give me a .csv valid file path.\")\n",
    "    landcover[long_column] = round(landcover[long_column], 6)\n",
    "    landcover[lat_column] = round(landcover[lat_column], 6)\n",
    "    landcover['x_rad'] = landcover[long_column].apply(lambda x: np.deg2rad(x))\n",
    "    landcover['y_rad'] = landcover[lat_column].apply(lambda x: np.deg2rad(x))\n",
    "    data['x_rad'] = data[long_column].apply(lambda x: np.deg2rad(x))\n",
    "    data['y_rad'] = data[lat_column].apply(lambda x: np.deg2rad(x))\n",
    "    ball = BallTree(landcover[[\"y_rad\", \"x_rad\"]].values, metric='haversine')\n",
    "    distances, indices = ball.query(data[[\"y_rad\", \"x_rad\"]].values, k = 1)\n",
    "    distances = [(d * 6371).tolist()[0] for d in distances]\n",
    "    indices = indices.tolist()\n",
    "    indices = [i[0] for i in indices]\n",
    "    del data['x_rad']\n",
    "    del data['y_rad']\n",
    "    del landcover['x_rad']\n",
    "    del landcover['y_rad']\n",
    "    del landcover[long_column]\n",
    "    del landcover[lat_column]\n",
    "    data['neighbors'] = indices\n",
    "    data = pd.merge(data, landcover, how='left',left_on = [data.neighbors], right_index=True)\n",
    "    del data['neighbors']\n",
    "    landcover_cols = landcover.columns.tolist()\n",
    "    data['landcover'] = data[date_column].apply(lambda x: str(x.year)+'_01_01_LC_Type1' if x.year <=2021 else '2021_01_01_LC_Type1')\n",
    "    data['landcover'] = data.apply(lambda x: x[x['landcover']], axis=1)\n",
    "    data = data.drop(columns=landcover_cols)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(dataframe,columns_list=[],columns_names = []):\n",
    "    \"\"\"Selects which columns to keep from the dataframe and optionally rename the columns \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Dataframe to be transformed\n",
    "        \n",
    "    columns_list : list, optional\n",
    "        A list with the names of the columns to keep (default = a list containing all columns)\n",
    "    \n",
    "    columns_names : list, optional\n",
    "        A list with the new names of the columns (default = a list containing the running names)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        A transformed dataframe\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "    If the length of columns_list and columns_names do not match\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(columns_list) != 0:\n",
    "            dataframe = dataframe[columns_list]\n",
    "        if len(columns_names) != 0:\n",
    "            dataframe.columns = columns_names\n",
    "    except:\n",
    "        raise KeyError('The column list and the name list must be of same size')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(dataframe,dupl_list=['x','y','dt_placement'],group_list=['x','y','dt_placement'],mosq_col='mosq_now'):\n",
    "    \"\"\"Removes the duplicates rows and aggragates observations needed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Dataframe to be transformed\n",
    "    \n",
    "    dupl_list : list\n",
    "        A list with the names of the columns for removing the duplicates upon them (default=['x','y','dt_placement'])\n",
    "        \n",
    "    group_list : list\n",
    "        A list with the names of the columns for grouping the duplicates upon them (default=['x','y','dt_placement'])\n",
    "    \n",
    "    mosq_col : str, optional\n",
    "        The name of the column with the mosquito number (default = 'mosq_now')\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        A transformed dataframe\n",
    "        \n",
    "    Raises\n",
    "    ------    \n",
    "    KeyError\n",
    "        If column name(s) given not in index\n",
    "    \"\"\"\n",
    "    \n",
    "    if (mosq_col not in dataframe.columns):\n",
    "        raise KeyError('Column(s) not in index')\n",
    "    if len(dupl_list) != 0:\n",
    "        for i in dupl_list:\n",
    "            if i not in dataframe.columns:\n",
    "                raise KeyError('Column(s) not in index')\n",
    "    dataframe.drop_duplicates(subset=dupl_list+[mosq_col], keep='first',inplace=True)\n",
    "    agg_dict = {mosq_col: lambda x: x.sum(min_count=1)}\n",
    "    col = [e for e in dataframe.columns if e not in [mosq_col]+group_list]\n",
    "    for i in col:\n",
    "        agg_dict[i]= 'first'\n",
    "    dataframe = dataframe.groupby(group_list).agg(agg_dict).reset_index()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(data, col_list, long_column='x', lat_column='y'):\n",
    "    \"\"\"Fills the NaN values of columns based on longitude and latitude column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Dataframe\n",
    "        Dataframe to be transformed\n",
    "    \n",
    "    col_list : list\n",
    "        A list with the names of the columns to complete\n",
    "        \n",
    "    long_column : str, , optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_column : str, , optional\n",
    "        The name of the column with the latitude (default = 'y')\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    data: Dataframe\n",
    "        A dataframe with filled nan values\n",
    "        \n",
    "    Raises\n",
    "    ------    \n",
    "    KeyError\n",
    "        If column name(s) given not in index\n",
    "    \"\"\"\n",
    "    for i in col_list+[long_column,lat_column]:\n",
    "        if i not in data.columns:\n",
    "            raise KeyError('Column(s) not in index')\n",
    "    stations = data[[long_column,lat_column]+col_list].drop_duplicates(subset=[long_column,lat_column])\n",
    "    data = data.drop(columns=col_list)\n",
    "    data = pd.merge(data, stations, how='left',left_on = [data[long_column],data[lat_column]],right_on = [stations[long_column],stations[lat_column]])\n",
    "    data = data.drop(columns=['key_0','key_1',long_column+'_y',lat_column+'_y'])\n",
    "    data = data.rename(columns={long_column+'_x':long_column, lat_column+'_x':lat_column})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_dataset(dataframe, fill_list):\n",
    "    \"\"\"Fills the NaN values of columns specified with spesific values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Dataframe to be transformed\n",
    "    \n",
    "    dupl_list : dict\n",
    "        A dictionairy with the names of the columns and the value for NaN to complete\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        A transformed dataframe with filled nan values\n",
    "        \n",
    "    Raises\n",
    "    ------    \n",
    "    KeyError\n",
    "        If column name(s) given not in index\n",
    "    \"\"\"\n",
    "    for i in list(fill_list.keys()):\n",
    "        if i not in dataframe.columns:\n",
    "            raise KeyError('Column(s) not in index')\n",
    "        else:\n",
    "            dataframe[i] = dataframe[i].fillna(fill_list[i])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_features(data, test, percentage = 0.3):\n",
    "    num_rows = len(test)\n",
    "    nan_features = test.isna().sum()\n",
    "    nan_features = nan_features/num_rows\n",
    "    nan_features = nan_features.drop(['mosq_sum_month', 'mosq_sum_month_previous_year', 'mosq_sum_year',\n",
    "                                     'mosq_sum_previous_2weeks', 'previous_mosq_measure','mosq_now'])\n",
    "    nan_features = nan_features[nan_features>percentage].index.tolist()\n",
    "    if len(nan_features) > 0:\n",
    "        print('Features Dropped:',nan_features)\n",
    "        data = data.drop(columns=nan_features)\n",
    "        test = test.drop(columns=nan_features)\n",
    "    return data, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unused_stations(data, test_df, period=None, threshold=None, date_col='dt_placement',):\n",
    "    \"\"\"Deletes from test set stations that are not used for a defined period, or that are used less than \n",
    "    5 times at the whole dataset to not give predictions for them.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    data : dataframe\n",
    "        A dataframe containing the data\n",
    "        \n",
    "    test_df : dataframe\n",
    "        A dataframe containing the prediction\n",
    "    \n",
    "    period : int\n",
    "        The number of previous years that a station is not used\n",
    "        \n",
    "    date_col : str, optional\n",
    "        The name of the date column (default = 'dt_placement')\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    test_df : dataframe\n",
    "        A dataframe with the most frequent stations\n",
    "    \"\"\" \n",
    "    # Calculate the year from which and then the check will be made for the existence of the station\n",
    "    if period is not None:\n",
    "        current_year = test_df[date_col].dt.to_period('Y').max().year\n",
    "        check_years = current_year - period\n",
    "        station_df = data.loc[data[date_col].dt.to_period('Y')>=str(check_years),['x','y']].drop_duplicates().reset_index(drop=True)\n",
    "        test_df = pd.merge(test_df, station_df, how='inner',on = ['x','y'])\n",
    "        \n",
    "    if threshold is not None:\n",
    "        count = data.groupby(['x','y'])[date_col].count().reset_index()\n",
    "        count = count.loc[count[date_col]>=threshold,['x','y']]\n",
    "        test_df = pd.merge(test_df, count, how='inner',on = ['x','y'])\n",
    "        \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff(data, long_column='x', lat_column='y', date_column='dt_placement'):\n",
    "    \"\"\"Creates a list with the time difference between two consecutive observations of each station\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Dataframe\n",
    "        The dataframe contaning the number of mosquitoes\n",
    "\n",
    "    long_column : str, optional\n",
    "        The name of the column with the longitude (default = 'x')\n",
    "        \n",
    "    lat_column : str, optional\n",
    "        The name of the column with the latitude (default = 'y')\n",
    "\n",
    "    date_column : str, optional\n",
    "        The name of the column containing the date of observations (default = 'dt_placement')\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    time_diff: lst\n",
    "        A list containing the distance of each observation from the next one\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "    If not some of the columns are included in the dataframe\n",
    "    \"\"\"\n",
    "    if (long_column not in data.columns or lat_column not in data.columns or date_column not in data.columns):\n",
    "        raise KeyError('Column(s) not in index')\n",
    "    time_diff = []\n",
    "    data = data.dropna(subset=['mosq_now'])\n",
    "    stations = data.loc[:, [long_column, lat_column]].drop_duplicates().reset_index(drop=True)\n",
    "    for i in range(len(stations)):\n",
    "        data2 = data.loc[(data[long_column] == stations.loc[i,long_column]) & (data[lat_column] == stations.loc[i,lat_column])]\n",
    "        data2 = data2.sort_values(by=[date_column], ascending=[True])\n",
    "        data2.reset_index(drop=True,inplace=True)\n",
    "        for j in range(len(data2)):\n",
    "            data3 = data2.loc[data2[date_column].dt.year == data2[date_column][j].year]\n",
    "            x = data3[date_column][j] < data3[date_column]\n",
    "            y = x[x==True]\n",
    "            if len(y) == 0:\n",
    "                y = np.nan\n",
    "            else:\n",
    "                y = x[x==True].idxmin()\n",
    "                y = np.abs((data2[date_column][j] - data2[date_column][y]).days)\n",
    "            time_diff.append(y)\n",
    "    time_diff = [x for x in time_diff if str(x) != 'nan']\n",
    "    time_diff.sort()\n",
    "    #print('Length: ',len(time_diff))\n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_plot(d, d_length):\n",
    "    \"\"\"Plots the cdf of the vector of days of difference between the observations of each station.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : Vector\n",
    "        A vector containing the days of differnece\n",
    "\n",
    "    d_length : int\n",
    "        The length of the vector d\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    step : lst\n",
    "        The optimal step in days in order to catch at least 80% of the observations days difference\n",
    "    \"\"\"\n",
    "    a = np.linspace(min(d), max(d), 100)\n",
    "    cdf = np.zeros(len(a))\n",
    "    for k, val in enumerate(a):\n",
    "        mask_d = d < val\n",
    "        cdf[k] = mask_d.sum()/ d_length\n",
    "\n",
    "    plt.plot(a,cdf)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Days Difference')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.show()\n",
    "    idx = (np. abs(cdf - 0.8)). argmin()\n",
    "    return np.round(a[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(data, mosq_column='mosq_now', threshold=10, date_column='dt_placement'):\n",
    "    \n",
    "    print('Total observatons:', len(data[['x','y','mosq_now']].dropna()))\n",
    "    print('Number of unique traps:', len(data[['x','y','mosq_now']].dropna().drop_duplicates(subset=['x','y'])))\n",
    "    print('Start date:', data[['mosq_now','dt_placement']].dropna()['dt_placement'].min())\n",
    "    print('End date:', data[['mosq_now','dt_placement']].dropna()['dt_placement'].max())\n",
    "    \n",
    "    all_obs = 0\n",
    "    data2 = data.dropna(subset=[mosq_column]).reset_index(drop=True)\n",
    "    \n",
    "    x2 = data2[data2[mosq_column] < np.percentile(data2[mosq_column], 95)].reset_index(drop=True)\n",
    "    plt.hist(x2[mosq_column],density=True)\n",
    "    plt.xlabel('Mosquito Abundance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"Mean:\",data2[mosq_column].mean())\n",
    "    print(\"Std:\",data2[mosq_column].std())\n",
    "    print(\"Skewness:\",skew(data2[mosq_column], bias=True))\n",
    "    print(\"kurtosis:\", kurtosis(data2[mosq_column], bias=True))\n",
    "    \n",
    "    max_year = data2['dt_placement'].max().year\n",
    "    for i in list(range(max_year-2, max_year+1)):\n",
    "        len_year = len(data2[data2[date_column].dt.year==i])\n",
    "        print(str(i)+' number of observations:',len_year)\n",
    "        all_obs = all_obs + len_year\n",
    "    print('All operational years observations:',all_obs)\n",
    "    \n",
    "    count = data[['x','y','mosq_now']].dropna()\n",
    "    count = count[['x','y']].groupby(['x','y']).size().reset_index(name='frequency')\n",
    "    count_frequent = len(count.loc[count['frequency']>=threshold])\n",
    "    count_infrequent = len(count.loc[count['frequency']<threshold])\n",
    "    print('Number of fixed traps:', count_frequent)\n",
    "    print('Number of temporal traps:', count_infrequent)\n",
    "    \n",
    "    time = calculate_diff(data)\n",
    "    time = np.array(time)\n",
    "    time = time[time < np.percentile(time, 95)]\n",
    "    step = cdf_plot(time, len(time))\n",
    "    plt.hist(time, density=True)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.ylabel('Days Difference')\n",
    "    plt.show()\n",
    "    \n",
    "    features = ['x', 'y', 'ndvi_mean', 'ndwi_mean', 'ndmi_mean', 'ndbi_mean', \n",
    "            'acc_rainfall_1week', 'acc_rainfall_2week', 'acc_rainfall_jan',\n",
    "            'lst_jan_day_mean', 'lst_feb_day_mean', 'lst_mar_day_mean',\n",
    "            'lst_apr_day_mean', 'lst_jan_night_mean', 'lst_feb_night_mean',\n",
    "            'lst_mar_night_mean', 'lst_apr_night_mean', 'DISTANCE_TO_COAST',\n",
    "            'DISTANCE_TO_RIVER', 'SLOPE_mean_1km', 'ASPECT_mean_200m',\n",
    "            'ELEVATION_mean_1km', 'HILLSHADE_mean_1km', 'FS_AREA_1km',\n",
    "            'FLOW_ACCU_200m', 'null_island_distance', 'vert_distance',\n",
    "            'days_distance', 'lst', 'lst_day', 'lst_night']\n",
    "    for f in features:\n",
    "        data_plot = x2[[f, mosq_column]].dropna().reset_index(drop=True)\n",
    "        plt.figure()\n",
    "        sns.kdeplot(data=data_plot, x=f, y=mosq_column, shade=True)\n",
    "        plt.xlabel(f)\n",
    "        plt.ylabel('Mosquito Abundance')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        x = data_plot[f]\n",
    "        y = data_plot[mosq_column]\n",
    "        nbins = 20\n",
    "        n, _ = np.histogram(x, bins=nbins)\n",
    "        sy, _ = np.histogram(x, bins=nbins, weights=y)\n",
    "        sy2, _ = np.histogram(x, bins=nbins, weights=y*y)\n",
    "        mean = sy / n\n",
    "        std = np.sqrt(sy2/n - mean*mean)\n",
    "        plt.plot(x, y, 'bo')\n",
    "        plt.errorbar((_[1:] + _[:-1])/2, mean, yerr=std, fmt='r-')\n",
    "        plt.xlabel(f)\n",
    "        plt.ylabel('Mosquito Abundance')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "#     data_plot = data[['dt_placement', 'lst', 'lst_day', 'lst_night', 'acc_rainfall_1week', 'acc_rainfall_2week', 'acc_rainfall_jan']]\n",
    "#     data_plot = data_plot.groupby('dt_placement').mean().reset_index()\n",
    "#     data_plot = data_plot.groupby(data_plot['dt_placement'].dt.to_period('M')).mean().drop(columns=['dt_placement']).reset_index()\n",
    "#     data_plot['dt_placement'] = data_plot['dt_placement'].dt.to_timestamp()\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(data_plot['dt_placement'], data_plot['lst'], label='Temperature ($^{o}$C)')\n",
    "#     plt.plot(data_plot['dt_placement'], data_plot['acc_rainfall_2week'], label='Accumulative Rainfall \\n 2 weeks(mm)')\n",
    "#     plt.grid()\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_rainfall_plot(data, date_column='dt_placement', temp_column='lst', rainfall_column='acc_rainfall_jan'):\n",
    "    \n",
    "    temp = data[[date_column,'lst_jan_day_mean',\n",
    "                 'lst_jan_night_mean', 'lst_feb_day_mean', 'lst_feb_night_mean',\n",
    "                 'lst_mar_day_mean', 'lst_mar_night_mean', 'lst_apr_day_mean',\n",
    "                 'lst_apr_night_mean']]\n",
    "\n",
    "    temp['lst_jan_mean'] = (temp['lst_jan_day_mean'] + temp['lst_jan_night_mean'])/2\n",
    "    temp['lst_feb_mean'] = (temp['lst_feb_day_mean'] + temp['lst_feb_night_mean'])/2\n",
    "    temp['lst_mar_mean'] = (temp['lst_mar_day_mean'] + temp['lst_mar_night_mean'])/2\n",
    "    temp['lst_apr_mean'] = (temp['lst_apr_day_mean'] + temp['lst_apr_night_mean'])/2\n",
    "    temp = temp.groupby(temp[date_column].dt.to_period('Y')).mean().drop(columns=[date_column]).reset_index()\n",
    "    temp[date_column] = temp[date_column].dt.to_timestamp()\n",
    "\n",
    "    temp_jan = temp[[date_column,'lst_jan_mean']].rename(columns={'lst_jan_mean':'lst'})\n",
    "    temp_jan[date_column] = temp_jan[date_column].apply(lambda dt: dt.replace(month=1))\n",
    "    temp_feb = temp[[date_column,'lst_feb_mean']].rename(columns={'lst_feb_mean':'lst'})\n",
    "    temp_feb[date_column] = temp_feb[date_column].apply(lambda dt: dt.replace(month=2))\n",
    "    temp_mar = temp[[date_column,'lst_mar_mean']].rename(columns={'lst_mar_mean':'lst'})\n",
    "    temp_mar[date_column] = temp_mar[date_column].apply(lambda dt: dt.replace(month=3))\n",
    "    temp_apr = temp[[date_column,'lst_apr_mean']].rename(columns={'lst_apr_mean':'lst'})\n",
    "    temp_apr[date_column] = temp_apr[date_column].apply(lambda dt: dt.replace(month=4))\n",
    "    temp = pd.concat([temp_jan, temp_feb, temp_mar, temp_apr],axis=0)\n",
    "\n",
    "    temp_daily = data[[date_column,'lst']]\n",
    "    temp_daily = temp_daily.groupby(date_column).mean().reset_index()\n",
    "    temp_daily = temp_daily.groupby(temp_daily[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    temp_daily[date_column] = temp_daily[date_column].dt.to_timestamp()\n",
    "\n",
    "    temp = pd.concat([temp,temp_daily],axis=0)\n",
    "    temp = temp.groupby(temp.dt_placement.dt.month)[temp_column].mean()\n",
    "\n",
    "    rain = data[[date_column,'acc_rainfall_1week', 'acc_rainfall_2week', 'acc_rainfall_jan']]\n",
    "    rain = rain[rain[date_column].dt.month!=2].reset_index(drop=True)\n",
    "    rain = rain.groupby(date_column).mean().reset_index()\n",
    "    rain = rain.groupby(rain[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    rain[date_column] = rain[date_column].dt.to_timestamp()\n",
    "\n",
    "    rain = rain.groupby(rain.dt_placement.dt.month)[rainfall_column].mean()\n",
    "    \n",
    "    months = {1: 'January',\n",
    "              2: 'February',\n",
    "              3:'March',\n",
    "              4:'April',\n",
    "              5:'May',\n",
    "              6:'June',\n",
    "              7:'July',\n",
    "              8:'August',\n",
    "              9:'September',\n",
    "              10:'October',\n",
    "              11:'November'}\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.bar(rain.index, rain, label='Rainfall')\n",
    "    ax2.plot(temp.index, temp, '-o', color='orange', label='Temperature')\n",
    "\n",
    "    ax1.set_ylabel('Accumulative Rainfall (mm)')\n",
    "    ax2.set_ylabel('Temperature ($^oC$)')\n",
    "\n",
    "    labels = [months[item] for item in temp.index]\n",
    "    ax1.set_xticks(temp.index)\n",
    "    ax1.set_xticklabels(labels,rotation=30)\n",
    "\n",
    "\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    colors = {'Rainfall':'#1f77b4'}         \n",
    "    labels.append('Rainfall')\n",
    "    handles.append(plt.Rectangle((0,0),1,1, color=colors['Rainfall']))\n",
    "    plt.legend(handles, labels)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    temp = data[[date_column,'lst_jan_day_mean',\n",
    "       'lst_jan_night_mean', 'lst_feb_day_mean', 'lst_feb_night_mean',\n",
    "       'lst_mar_day_mean', 'lst_mar_night_mean', 'lst_apr_day_mean',\n",
    "       'lst_apr_night_mean']]\n",
    "\n",
    "    temp['lst_jan_mean'] = (temp['lst_jan_day_mean'] + temp['lst_jan_night_mean'])/2\n",
    "    temp['lst_feb_mean'] = (temp['lst_feb_day_mean'] + temp['lst_feb_night_mean'])/2\n",
    "    temp['lst_mar_mean'] = (temp['lst_mar_day_mean'] + temp['lst_mar_night_mean'])/2\n",
    "    temp['lst_apr_mean'] = (temp['lst_apr_day_mean'] + temp['lst_apr_night_mean'])/2\n",
    "    temp = temp.groupby(temp[date_column].dt.to_period('Y')).mean().drop(columns=[date_column]).reset_index()\n",
    "    temp[date_column] = temp[date_column].dt.to_timestamp()\n",
    "\n",
    "    temp_jan = temp[[date_column,'lst_jan_mean']].rename(columns={'lst_jan_mean':'lst'})\n",
    "    temp_jan[date_column] = temp_jan[date_column].apply(lambda dt: dt.replace(month=1))\n",
    "    temp_feb = temp[[date_column,'lst_feb_mean']].rename(columns={'lst_feb_mean':'lst'})\n",
    "    temp_feb[date_column] = temp_feb[date_column].apply(lambda dt: dt.replace(month=2))\n",
    "    temp_mar = temp[[date_column,'lst_mar_mean']].rename(columns={'lst_mar_mean':'lst'})\n",
    "    temp_mar[date_column] = temp_mar[date_column].apply(lambda dt: dt.replace(month=3))\n",
    "    temp_apr = temp[[date_column,'lst_apr_mean']].rename(columns={'lst_apr_mean':'lst'})\n",
    "    temp_apr[date_column] = temp_apr[date_column].apply(lambda dt: dt.replace(month=4))\n",
    "    temp = pd.concat([temp_jan, temp_feb, temp_mar, temp_apr],axis=0)\n",
    "\n",
    "    temp_daily = data[[date_column,'lst']]\n",
    "    temp_daily = temp_daily.groupby(date_column).mean().reset_index()\n",
    "    temp_daily = temp_daily.groupby(temp_daily[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    temp_daily[date_column] = temp_daily[date_column].dt.to_timestamp()\n",
    "    temp = pd.concat([temp,temp_daily],axis=0)\n",
    "\n",
    "    temp_2021 = temp[temp[date_column].dt.year==2021]\n",
    "    temp_2021 = temp_2021.groupby(temp_2021.dt_placement).mean().reset_index()\n",
    "\n",
    "    temp_2022 = temp[temp[date_column].dt.year==2022]\n",
    "    temp_2022 = temp_2022.groupby(temp_2022.dt_placement).mean().reset_index()\n",
    "\n",
    "    temp_2023 = temp[temp[date_column].dt.year==2023]\n",
    "    temp_2023 = temp_2023.groupby(temp_2023.dt_placement).mean().reset_index()\n",
    "    temp_2023\n",
    "\n",
    "    temp = temp[temp[date_column].dt.year<2021]\n",
    "    temp = temp.groupby(temp.dt_placement.dt.month)[temp_column].mean()\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(temp_2021.dt_placement.dt.month, temp_2021[temp_column], '-o', label='2021')\n",
    "    ax1.plot(temp_2022.dt_placement.dt.month, temp_2022[temp_column], '-o', label='2022')\n",
    "    ax1.plot(temp_2023.dt_placement.dt.month, temp_2023[temp_column], '-o', label='2023')\n",
    "    ax1.plot(temp.index, temp, '--', label='2010-2020 mean')\n",
    "\n",
    "\n",
    "    labels = [months[item] for item in temp.index]\n",
    "    ax1.set_xticks(temp.index)\n",
    "    ax1.set_xticklabels(labels,rotation=30)\n",
    "\n",
    "    plt.ylabel('Temperature ($^oC$)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    rain = data[[date_column,'acc_rainfall_1week', 'acc_rainfall_2week', 'acc_rainfall_jan']]\n",
    "    rain = rain[rain[date_column].dt.month!=2].reset_index(drop=True)\n",
    "    rain = rain.groupby(date_column).mean().reset_index()\n",
    "    rain = rain.groupby(rain[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    rain[date_column] = rain[date_column].dt.to_timestamp()\n",
    "\n",
    "    rain_2021 = rain[rain[date_column].dt.year==2021]\n",
    "    rain_2022 = rain[rain[date_column].dt.year==2022]\n",
    "    rain_2023 = rain[rain[date_column].dt.year==2023]\n",
    "\n",
    "    rain = rain[rain[date_column].dt.year<2021]\n",
    "    rain = rain.groupby(rain.dt_placement.dt.month)[rainfall_column].mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(rain_2021.dt_placement.dt.month, rain_2021[rainfall_column], '-o', label='2021')\n",
    "    ax1.plot(rain_2022.dt_placement.dt.month, rain_2022[rainfall_column], '-o', label='2022')\n",
    "    ax1.plot(rain_2023.dt_placement.dt.month, rain_2023[rainfall_column], '-o', label='2023')\n",
    "    ax1.plot(rain.index, rain, '--', label='2010-2020 mean')\n",
    "\n",
    "    labels = [months[item] for item in rain.index]\n",
    "    ax1.set_xticks(rain.index)\n",
    "    ax1.set_xticklabels(labels,rotation=30)\n",
    "\n",
    "    plt.ylabel('Accumulative Rainfall (mm)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    data2 = data[data['mosq_now'] < np.percentile(data['mosq_now'].dropna(), 60)].reset_index(drop=True)\n",
    "    \n",
    "    mosq = data[[date_column,'mosq_now']]\n",
    "    mosq = mosq[mosq[date_column].dt.month!=2].reset_index(drop=True)\n",
    "    mosq = mosq.groupby(date_column).mean().reset_index()\n",
    "    mosq = mosq.groupby(mosq[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    mosq[date_column] = mosq[date_column].dt.to_timestamp()\n",
    "    mosq = mosq.groupby(mosq.dt_placement.dt.month)['mosq_now'].mean()\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(mosq.index, mosq, '-o', label='mosquito population')\n",
    "\n",
    "    labels = [months[item] for item in temp.index]\n",
    "    ax1.set_xticks(temp.index)\n",
    "    ax1.set_xticklabels(labels,rotation=30)\n",
    "\n",
    "    plt.ylabel('Mosquito Population')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    mosq = data[[date_column,'mosq_now']]\n",
    "    mosq = mosq[mosq[date_column].dt.month!=2].reset_index(drop=True)\n",
    "    mosq = mosq.groupby(date_column).mean().reset_index()\n",
    "    mosq = mosq.groupby(mosq[date_column].dt.to_period('M')).mean().drop(columns=[date_column]).reset_index()\n",
    "    mosq[date_column] = mosq[date_column].dt.to_timestamp()\n",
    "    \n",
    "    mosq_2021 = mosq[mosq[date_column].dt.year==2021]\n",
    "    mosq_2022 = mosq[mosq[date_column].dt.year==2022]\n",
    "    mosq_2023 = mosq[mosq[date_column].dt.year==2023]\n",
    "\n",
    "    mosq = mosq[mosq[date_column].dt.year<2021]\n",
    "    mosq = mosq.groupby(mosq.dt_placement.dt.month)['mosq_now'].mean()\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax1.plot(mosq_2021.dt_placement.dt.month, mosq_2021['mosq_now'], '-o', label='2021')\n",
    "    ax1.plot(mosq_2022.dt_placement.dt.month, mosq_2022['mosq_now'], '-o', label='2022')\n",
    "    ax1.plot(mosq_2023.dt_placement.dt.month, mosq_2023['mosq_now'], '-o', label='2023')\n",
    "    ax1.plot(mosq.index, mosq, '--', label='2010-2020 mean')\n",
    "\n",
    "    labels = [months[item] for item in temp.index]\n",
    "    ax1.set_xticks(temp.index)\n",
    "    ax1.set_xticklabels(labels,rotation=30)\n",
    "\n",
    "    plt.ylabel('Mosquito Population')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(data, mosq_column='mosq_now', date_column='dt_placement'):\n",
    "    \n",
    "    drop_columns1 = [date_column]\n",
    "    drop_columns2 = [date_column, 'mosq_sum_month',\n",
    "                    'mosq_sum_month_previous_year', 'mosq_sum_year',\n",
    "                    'mosq_sum_previous_2weeks', 'previous_mosq_measure','year']\n",
    "\n",
    "    plt.figure(figsize=(10,15))\n",
    "    sns.heatmap(data.drop(columns=drop_columns1).corr()[[mosq_column]].sort_values(by=[mosq_column])[:-1], annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    abs_corr = np.abs(data.drop(columns=drop_columns2).corr()[[mosq_column]]).sort_values(by=[mosq_column],ascending=False)[1:]\n",
    "    zz = data.drop(columns=drop_columns2).groupby(data[date_column].dt.year).corr()[mosq_column]\n",
    "    for i in abs_corr.index[:3]:\n",
    "        plt.plot(zz.index.get_level_values(0).unique().sort_values().tolist(), zz.xs(i, level=1, drop_level=False).values, '-o', label=i)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
