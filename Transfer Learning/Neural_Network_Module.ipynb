{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d653348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6d570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(target,num):\n",
    "    \"\"\"Creates an one hot encoded vector based on the class of each observation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target : pd.Series\n",
    "        The series containing the target variable to be transformed to vector\n",
    "        \n",
    "    num_classes : int\n",
    "        The number of different classes\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    vectors : pd.Series\n",
    "        A pd.Series object with one hot encoded vectors\n",
    "\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(sparse=False,categories = [list(range(num))],handle_unknown='ignore')\n",
    "    enc.fit(np.array(target).reshape(-1, 1))\n",
    "    target= target.apply(lambda x: enc.transform(np.array(x).reshape(1, -1))[0])\n",
    "    target = np.stack(target)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1b7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(y,min_val,max_val):\n",
    "    'Transforms the target data to 0-1 range in order to use the sigmoid function as activation'\n",
    "    return ((np.array(y) - min_val) / (max_val-min_val))\n",
    "\n",
    "def logData(y):\n",
    "    'Transforms the target variable to log(target) in order to follow a more normal disribution'\n",
    "    return np.log1p(y)\n",
    "\n",
    "def expData(data):\n",
    "    'Calculate the exponential value of the target variable'\n",
    "    return(np.expm1(data))\n",
    "\n",
    "def denormalizeData(data,min_val,max_val):\n",
    "    'Transforms the data from 0-1 range to the initial 0-9 range'\n",
    "    return((np.array(data)*(max_val-min_val))+min_val).tolist()\n",
    "\n",
    "def sigmoid(x,a=1,b=0):\n",
    "    return 1.0 / (1.0 + np.exp(a*(-x+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a57a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, train, evals, ylabel):\n",
    "    \"\"\"Prints the plot of evaluation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : list\n",
    "        A list with the epochs\n",
    "        \n",
    "    train : list\n",
    "        A list with the prediction score on the train set\n",
    "        \n",
    "    evals : list\n",
    "        A list with the prediction score on the test set\n",
    "        \n",
    "    y_label : str\n",
    "        The label of the y-axis\n",
    "    \"\"\"\n",
    "    for i in range(len(train)):\n",
    "        if evals[i] > train[i]:\n",
    "            break\n",
    "    plt.plot(epochs, train, label='Train')\n",
    "    plt.plot(epochs, evals, label='Eval')\n",
    "    plt.vlines(x = i+1,ls='--', ymin = 0, ymax = max(max(train),max(evals)), colors = 'grey', label = 'x = '+str(i+1))\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations(data, model_type, test=None, evaluation=False, transformation_list=['scaling'], embeddings=None):\n",
    "    \"\"\"Executes scaling, augmentation or label endconding on the dataset \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        A dataframe containing all the data or the data of trainning.\n",
    "        If test not given, then data will be randomnly slpit in 80-20% train and test set\n",
    "        \n",
    "    model_type : str\n",
    "        The type of the model to be implemented.\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    test : dataframe, optional\n",
    "        A dataframe containing all the data for testing (default = None)\n",
    "        \n",
    "    scaling : boolean, optional\n",
    "        If True, perofrms scaling on numerical features (default = False)\n",
    "        \n",
    "    augment : boolean, optional\n",
    "        If True, augments the train data with existing observations\n",
    "        and giving greater weight on the observations with greater target value (default = False)\n",
    "        \n",
    "    embeddings : list, optional\n",
    "        A list of columns with categorical features in order to be label encoded (default = None)\n",
    "        \n",
    "    evaluation : boolean, optional\n",
    "        If True, 20% of the observations of train set will be held for evaluation set (default = False)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    train_X: numpy array\n",
    "        A numpy array with independent variables for training\n",
    "        \n",
    "    train_y: pd.Series\n",
    "        A array with the dependent variables (target) for training\n",
    "        \n",
    "    test_X: numpy array\n",
    "        A numpy array with independent variables for test\n",
    "        \n",
    "    test_y: pd.Series\n",
    "        A array with the dependent variables (target) for test\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if test is None:    \n",
    "        X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        train_X,test_X,train_y,test_y = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    else:\n",
    "        data = data.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "        train_X, train_y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        test_X, test_y = test.iloc[:,:-1], test.iloc[:,-1]\n",
    "        \n",
    "    train_X = train_X.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    test_X = test_X.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "        \n",
    "    if evaluation:\n",
    "        train_X,eval_X,train_y,eval_y = train_test_split(train_X, train_y, test_size=0.20, random_state=1)\n",
    "        eval_X = eval_X.reset_index(drop=True)\n",
    "        eval_y = eval_y.reset_index(drop=True)\n",
    "            \n",
    "    percentile = round(np.percentile(train_y, 95))\n",
    "    train_y.loc[train_y >= percentile] = percentile\n",
    "\n",
    "    if 'augmentation' in transformation_list:\n",
    "        augment_index = train_y.sample(frac=0.4, weights=train_y, random_state=1, replace=True).index\n",
    "        train_X = pd.concat([train_X,train_X.iloc[augment_index,:]]).reset_index(drop=True)\n",
    "        train_y = pd.concat([train_y,train_y[augment_index]]).reset_index(drop=True)\n",
    "    \n",
    "    if model_type == 'classification':\n",
    "        num_classes = len(pd.concat([train_y,test_y]).unique())\n",
    "        if evaluation:\n",
    "            num_classes = len(pd.concat([train_y, test_y, eval_y]).unique())\n",
    "            eval_y = vectorise(eval_y,num_classes)               \n",
    "        train_y = vectorise(train_y,num_classes)\n",
    "        test_y = vectorise(test_y,num_classes)\n",
    "\n",
    "    if embeddings != None:   \n",
    "        embedding_cols = embeddings.columns.tolist()\n",
    "        embedded_columns_train = train_X.loc[:,embedding_cols] #categorical columns\n",
    "        train_X = train_X.drop(columns=embedding_cols)\n",
    "\n",
    "        embedded_columns_test = test_X.loc[:,embedding_cols] #categorical columns\n",
    "        test_X = test_X.drop(columns=embedding_cols)\n",
    "        \n",
    "        if evaluation:\n",
    "            embedded_columns_eval = eval_X.loc[:,embedding_cols] #categorical columns\n",
    "            eval_X = eval_X.drop(columns=embedding_cols)\n",
    "            \n",
    "        label_encoder = LabelEncoder()\n",
    "        for col in embeddings:\n",
    "            label_encoder.fit(pd.concat([embedded_columns_train[col],embedded_columns_test[col]],axis=0))\n",
    "            if evaluation:\n",
    "                label_encoder.fit(pd.concat([embedded_columns_train[col],embedded_columns_test[col],embedded_columns_eval[col]],axis=0))\n",
    "                embedded_columns_eval[col] = label_encoder.transform(embedded_columns_eval[col])\n",
    "            embedded_columns_train[col] = label_encoder.transform(embedded_columns_train[col])\n",
    "            embedded_columns_test[col] = label_encoder.transform(embedded_columns_test[col])\n",
    "\n",
    "        train_X_emb = embedded_columns_train.values\n",
    "        test_X_emb = embedded_columns_test.values\n",
    "        if evaluation:\n",
    "            eval_X_emb = embedded_columns_eval.values\n",
    "            \n",
    "    if 'scaling' in transformation_list:\n",
    "        scaler = StandardScaler()\n",
    "        train_X = scaler.fit_transform(train_X)\n",
    "        test_X = scaler.transform(test_X)\n",
    "        if evaluation:\n",
    "            eval_X = scaler.transform(eval_X)\n",
    "            \n",
    "    if 'normalization' in transformation_list:\n",
    "        min_val = train_y.min()\n",
    "        max_val = train_y.max()\n",
    "        test_y = normalizeData(test_y, min_val, max_val)\n",
    "        train_y = normalizeData(train_y, min_val, max_val)\n",
    "        if evaluation:\n",
    "            eval_y = normalizeData(eval_y, min_val, max_val)\n",
    "            \n",
    "    if 'log' in transformation_list:\n",
    "        test_y = logData(test_y)\n",
    "        train_y = logData(train_y)\n",
    "        if evaluation:\n",
    "            eval_y = logData(eval_y)\n",
    "            \n",
    "    if embeddings != None:\n",
    "        train_X = [train_X,train_X_emb]\n",
    "        test_X = [test_X, test_X_emb]\n",
    "        if evaluation:\n",
    "            eval_X = [eval_X,eval_X_emb]\n",
    "    else:\n",
    "        train_X = [train_X]\n",
    "        test_X = [test_X]\n",
    "        if evaluation:\n",
    "            eval_X = [eval_X]\n",
    "            \n",
    "    if evaluation:\n",
    "        return train_X, train_y, eval_X, eval_y, test_X, test_y\n",
    "    else:       \n",
    "        return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    l1_weight: int or float, optional\n",
    "        Weight parameter for L1 regularization (default=0)\n",
    "    \n",
    "    l2_weight: int or float, optional\n",
    "        Weight parameter for L2 regularization (default=0)\n",
    "    \"\"\"    \n",
    "    def __init__(self, output_len, hidden_layers, model_type, learning_rate, epochs, batch_size, embedding_data=None,\n",
    "                 dropout=None, transformation_list=[], early_stop=False, l1_weight=0, l2_weight=0, val_metrics=[]):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if (isinstance(dropout, float)) and (dropout < 0 or dropout > 1):\n",
    "            raise ValueError('Dropout rate argument must be a float number in [0,1]')\n",
    "        elif isinstance(dropout, list):\n",
    "            if any(i<0 or i>1 for i in dropout):\n",
    "                raise ValueError('All elements of the dropoutlist must be 0<=1<=1')\n",
    "            if (embedding_data is None) and (len(dropout)!=len(hidden_layers)):\n",
    "                raise ValueError('Dropout list and hidden_layers list must be of the same size')\n",
    "            if (embedding_data is not None) and (len(dropout)!=len(hidden_layers)+1):\n",
    "                raise ValueError('Dropout list must be one element greater than the hidden layers list')\n",
    "        \n",
    "        self.output_len = output_len\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.model_type = model_type\n",
    "        self.transformation_list = transformation_list\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.early_stop = early_stop\n",
    "        self.embedding_data = embedding_data\n",
    "        self.dropout = dropout\n",
    "        self.l1_weight = l1_weight\n",
    "        self.l2_weight = l2_weight\n",
    "        if isinstance(early_stop, tuple):\n",
    "            self.early_stop = tf.keras.callbacks.EarlyStopping(monitor=early_stop[0],min_delta=early_stop[1],patience=early_stop[2])\n",
    "        self.val_metrics = val_metrics\n",
    "        \n",
    "        tf.keras.utils.set_random_seed(0)\n",
    "        self.activation_relu = tf.keras.layers.Activation(tf.keras.activations.relu)\n",
    "        self.activation_softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n",
    "        self.activation_sigmoid = tf.keras.layers.Activation(tf.keras.activations.sigmoid)\n",
    "        self.initializer = tf.keras.initializers.HeNormal()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        if self.model_type == 'classification':\n",
    "            self.loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        else:\n",
    "            self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.linear_layers = [tf.keras.layers.Dense(hidden_layers[i], kernel_initializer=self.initializer,kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1_weight, l2=l2_weight)) for i in range(len(hidden_layers))]\n",
    "        self.batchnorm_layers = [tf.keras.layers.BatchNormalization() for i in range(len(self.linear_layers))]\n",
    "        self.dropout_layers =[]\n",
    "    \n",
    "        if embedding_data is not None:\n",
    "            self.embedding_layers = [tf.keras.layers.Embedding(input_dim=len(embedding_data.iloc[:,i].unique()),output_dim=min(10,int(len(embedding_data.iloc[:,i].unique())/2)),input_length=1) for i in range(len(embedding_data.iloc[0,:]))]\n",
    "        \n",
    "        if dropout != None:\n",
    "            if embedding_data is not None:\n",
    "                dropout =[dropout for i in range(len(self.linear_layers)+1)]\n",
    "            else:\n",
    "                dropout =[dropout for i in range(len(self.linear_layers))]   \n",
    "        \n",
    "            self.dropout_layers = [tf.keras.layers.Dropout(dropout[i]) for i in range(len(dropout))]\n",
    "        \n",
    "        self.linear_layers.append(tf.keras.layers.Dense(output_len, kernel_initializer=self.initializer,kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1_weight, l2=l2_weight)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs_cont=inputs[0]\n",
    "        dropout_layers = self.dropout_layers\n",
    "        \n",
    "        if len(inputs)==2:\n",
    "            cat=[]\n",
    "            inputs_cat = inputs[1]\n",
    "            for i in range(len(self.embedding_layers)):\n",
    "                if tf.shape(inputs_cat).shape[0]>1:\n",
    "                    x = self.embedding_layers[i](inputs_cat[:,i])\n",
    "                else:\n",
    "                    x = self.embedding_layers[i](inputs_cat)\n",
    "                if len(self.dropout_layers) != 0:    \n",
    "                    x = dropout_layers[0](x)\n",
    "                    dropout_layers = dropout_layers[1:]\n",
    "                cat.append(x)\n",
    "            combined = tf.keras.layers.concatenate([cat[i] for i in range(len(cat))])\n",
    "            inputs_cont = tf.keras.layers.concatenate([combined,inputs_cont], axis=1)\n",
    "            \n",
    "        x = self.linear_layers[0](inputs_cont)\n",
    "        x = self.activation_relu(x)\n",
    "        if (len(dropout_layers) != 0):\n",
    "            x = dropout_layers[0](x)\n",
    "\n",
    "        for i in range(1,len(self.linear_layers)-1):\n",
    "            x = self.batchnorm_layers[i-1](x)\n",
    "            x = self.linear_layers[i](x)\n",
    "            x = self.activation_relu(x)\n",
    "            if (len(dropout_layers) != 0):\n",
    "                x = dropout_layers[i](x)\n",
    "                \n",
    "        x = self.batchnorm_layers[-1](x)        \n",
    "        x = self.linear_layers[-1](x)\n",
    "        if self.model_type=='classification':\n",
    "            x = self.activation_softmax(x)\n",
    "        else:\n",
    "            if 'normalization' in self.transformation_list:\n",
    "                x = self.activation_sigmoid(x)\n",
    "            else:\n",
    "                x = self.activation_relu(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_len\": self.output_len,\n",
    "                \"hidden_layers\": self.hidden_layers,\n",
    "                \"model_type\" : self.model_type,\n",
    "                \"transformation_list\": self.transformation_list,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"early_stop\": self.early_stop,\n",
    "                \"embedding_data\": self.embedding_data,\n",
    "                \"dropout\": self.dropout,\n",
    "                \"l1_weight\": self.l1_weight,\n",
    "                \"l2_weight\": self.l2_weight,\n",
    "                \"val_metrics\": self.val_metrics\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006cad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_X, train_y, test_X, test_y, max_val, learning_rate=None, \n",
    "             epochs=None, batch_size=None, early_stop=False, sample_weights=False):\n",
    "    \"\"\" Trainning of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A FeedforwardNeuralNetModel model\n",
    "        \n",
    "    train_X : Dataset\n",
    "        A Dataset object with the train set\n",
    "        \n",
    "    test_set : Dataset\n",
    "        A Dataset object with the test set\n",
    "        \n",
    "    learning_rate : int, optional\n",
    "        The learning_rate of the training process. (default = None)\n",
    "        \n",
    "    epochs : int, optional\n",
    "        The number of epochs for the training. (default = None)\n",
    "        \n",
    "    batch_size : int, optional\n",
    "        The size of each batch in each iteration. (default = None)\n",
    "        \n",
    "    ealry_stop : boolean, optional\n",
    "        If True, the trainning of the model may stop earlier than the epochs defined. (default = None)\n",
    "        \n",
    "    sample_weights: boolean, optional\n",
    "        If True, each sample is weighted based on the mosquito number (default = False)\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    results_train: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the train set\n",
    "        \n",
    "    results_test: DataFrame\n",
    "        A Dataframe containing the actual and the predicted values on the test set\n",
    "        \n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A trained FeedforwardNeuralNetModel model \n",
    "    \"\"\"    \n",
    "    if epochs == None:\n",
    "        epochs = model.epochs\n",
    "    \n",
    "    if learning_rate == None:\n",
    "        learning_rate = model.learning_rate\n",
    "        \n",
    "    if batch_size == None:\n",
    "        batch_size = model.batch_size\n",
    "        \n",
    "    if early_stop == False:\n",
    "        early_stop = model.early_stop\n",
    "    \n",
    "    model.compile(loss = model.loss, optimizer = model.optimizer, metrics = model.val_metrics)\n",
    "    \n",
    "    if sample_weights:\n",
    "        weights = np.array([sigmoid(train_y[i],0.005,1) for i in range(len(train_y))])\n",
    "    else:\n",
    "        weights = np.array([1 for i in range(len(train_y))])\n",
    "        \n",
    "    if early_stop == False:\n",
    "        history = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n",
    "                            sample_weight=weights, validation_data=(test_X,test_y))\n",
    "    else:\n",
    "        history = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n",
    "                            sample_weight=weights, validation_data=(test_X,test_y), callbacks=[early_stop])\n",
    "        epochs = early_stop.stopped_epoch + 1\n",
    "\n",
    "    for i in ['loss'] + model.val_metrics:\n",
    "        my_plot(np.linspace(1, epochs, epochs).astype(int), history.history[i], history.history['val_'+i] ,i)\n",
    "\n",
    "    train_predict = model.predict(train_X)\n",
    "    test_predict = model.predict(test_X)\n",
    "       \n",
    "    \n",
    "    if model.model_type == 'classification':\n",
    "        train_predict = np.argmax(train_predict, axis=1)\n",
    "        test_predict = np.argmax(test_predict, axis=1)\n",
    "        train_y = np.argmax(train_y, axis=1)\n",
    "        test_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "    else:\n",
    "        if 'normalization' in model.transformation_list:\n",
    "            train_y = denormalizeData(train_y,min_val=0,max_val=max_val)\n",
    "            train_predict = denormalizeData(train_predict,min_val=0,max_val=max_val)\n",
    "            test_y = denormalizeData(test_y,min_val=0,max_val=max_val)\n",
    "            test_predict = denormalizeData( test_predict,min_val=0,max_val=max_val)\n",
    "            \n",
    "        if 'log' in model.transformation_list:\n",
    "            train_y = expData(train_y)\n",
    "            train_predict = expData(train_predict)\n",
    "            test_y = expData(test_y)\n",
    "            test_predict = expData(test_predict)\n",
    "            \n",
    "        train_y = np.round(train_y)\n",
    "        train_predict = [np.round(e[0]) for e in train_predict]\n",
    "        test_y = np.round(test_y)\n",
    "        test_predict = [np.round(e[0]) for e in test_predict]\n",
    "\n",
    "        \n",
    "    results_train = {'actual': train_y, 'prediction': train_predict}\n",
    "    results_train = pd.DataFrame.from_dict(results_train)\n",
    "    results_test = {'actual': test_y, 'prediction': test_predict}\n",
    "    results_test = pd.DataFrame.from_dict(results_test)\n",
    "    \n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "    results_train.loc[results_train['prediction'] < 0,'prediction'] = 0\n",
    "    results_train.loc[results_train['prediction'] > max_val,'prediction'] = max_val\n",
    "        \n",
    "    return results_train, results_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49137cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_predictions(model, test_X, max_val, test_y=None):\n",
    "    \"\"\" Returns predictions of a nn model on a set of featues.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : FeedforwardNeuralNetModel\n",
    "        A trained FeedforwardNeuralNetModel model \n",
    "        \n",
    "    test_set : Dataset\n",
    "        A Dataset object with the test set\n",
    "                \n",
    "    Returns\n",
    "    ----------\n",
    "    test_predict: list\n",
    "        A list of the predictions for a test set given\n",
    "        \n",
    "    \"\"\"    \n",
    "\n",
    "    test_predict = model.predict(test_X)\n",
    "       \n",
    "    if model.model_type == 'classification':\n",
    "        test_predict = np.argmax(test_predict, axis=1)\n",
    "        if test_y != None:\n",
    "            test_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "    else:\n",
    "        if 'normalization' in model.transformation_list:\n",
    "            if test_y != None:\n",
    "                test_y = denormalizeData(test_y,min_val=0,max_val=max_val)\n",
    "            test_predict = denormalizeData(test_predict,min_val=0,max_val=max_val)\n",
    "            \n",
    "        if 'log' in model.transformation_list:\n",
    "            if test_y != None:\n",
    "                test_y = expData(test_y)\n",
    "            test_predict = expData(test_predict)\n",
    "            \n",
    "        if test_y != None:\n",
    "            test_y = np.round(test_y)\n",
    "        test_predict = [np.round(e[0]) for e in test_predict]\n",
    "  \n",
    "    if test_y != None:    \n",
    "        results_test = {'actual': test_y, 'prediction': test_predict}\n",
    "        results_test = pd.DataFrame.from_dict(results_test)\n",
    "    else:\n",
    "        results_test = {'prediction': test_predict}\n",
    "        results_test = pd.DataFrame.from_dict(results_test)\n",
    "        \n",
    "    results_test.loc[results_test['prediction'] < 0,'prediction'] = 0\n",
    "    results_test.loc[results_test['prediction'] > max_val,'prediction'] = max_val\n",
    "        \n",
    "    return results_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
