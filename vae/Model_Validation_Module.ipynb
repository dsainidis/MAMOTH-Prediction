{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5151e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.optimize import curve_fit\n",
    "from heapq import nsmallest\n",
    "from sklearn.manifold import TSNE\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a109430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucleidian(X_train, X_test, M=None, weights=None, same_array=False):\n",
    "    distances = []\n",
    "    \n",
    "    if M is None:\n",
    "        M = X_train.shape[0]\n",
    "        \n",
    "    for i in range(len(X_test)):\n",
    "        dist = []\n",
    "        for j in range(len(X_train)):\n",
    "            diff = X_test[i] - X_train[j]\n",
    "            d = np.sqrt(np.dot(diff.T, diff))\n",
    "            dist.append(d)\n",
    "        if weights is not None:\n",
    "            dist = [a*b for a,b in zip(dist, weights)]\n",
    "        if same_array:\n",
    "            del dist[i]\n",
    "        distances.append(mean(nsmallest(M, dist)))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_dist(actual,predictions, model_type, case=''):\n",
    "    \"\"\"Prints the error distribution plot\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "        \n",
    "    model_type : str\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "    \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "    \"\"\"\n",
    "    error = np.abs(actual-predictions).tolist()\n",
    "    if model_type == 'classification' or model_type == 'class_regression':\n",
    "        bins = np.arange(len(actual.unique())) - 0.5\n",
    "        plt.hist(error, bins)\n",
    "        plt.xticks(range(len(actual.unique())))\n",
    "    else:\n",
    "        plt.hist(error)\n",
    "    plt.xlabel('abs(error)')\n",
    "    plt.title('Error Distribution \\n' + case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4c3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_per_class(actual,predictions, case=''):\n",
    "    \"\"\"Prints the error distribution per class plot\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "        \n",
    "    model_type : str\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "    \"\"\"\n",
    "    actual = pd.Series(actual)\n",
    "    predictions = pd.Series(predictions)\n",
    "    labels = actual.unique().tolist()\n",
    "    labels.sort()\n",
    "    f = []\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "\n",
    "    for k in labels:\n",
    "        index = actual.loc[actual==k].index\n",
    "        actual_p = actual.iloc[index]\n",
    "        predictions_p = predictions.iloc[index]\n",
    "        mae_class = mean_absolute_error(actual_p, predictions_p)\n",
    "        f.append(mae_class)\n",
    "    labels = [str(int(e)) for e in labels]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(labels,f)\n",
    "    for i, v in enumerate(f):\n",
    "        ax.text(i, v, str('%.2f'%(v)),rotation=30)\n",
    "    plt.xlabel('class')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('MAE per class \\n'+case)\n",
    "    plt.show()\n",
    "    \n",
    "    print('-----------|class error-MAE| difference-----------')\n",
    "    z = np.abs(f-mean_absolute_error(actual, predictions))\n",
    "    print('mean:',z.mean())\n",
    "    print('std:',z.std())\n",
    "    print('coefficient of variation (std/mean):',z.std()/z.mean())\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('----------normalized difference-------------')\n",
    "    min_val = min(z)\n",
    "    max_val = max(z)\n",
    "    z = (z - min_val) / (max_val-min_val)\n",
    "    print('mean:',z.mean())\n",
    "    print('std:',z.std())\n",
    "    print('coefficient of variation (std/mean):',z.std()/z.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983fdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_per_month(df, case=''):\n",
    "    \"\"\"Prints the error per month\n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        A dataframe containing the data\n",
    "    \n",
    "    case: str, optional\n",
    "        Title of the plot (Area and mosquito genus) (default= '')\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    labels = (df['dt_prediction'].dt.month).unique()\n",
    "    labels.sort()\n",
    "    labels = [str(e) for e in labels]\n",
    "    f = df.groupby(by=[df['dt_prediction'].dt.month])['abs(error)'].mean().values\n",
    "    ax.bar(labels,f)\n",
    "    for i, v in enumerate(f):\n",
    "         ax.text(i, v, str('%.2f'%(v)),rotation=30)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Mean Absolute Error per month \\n' + case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919b807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_error(actual,predictions, case=''):\n",
    "    \"\"\"Prints the error in relation with the distance of point from the train region\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "    \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "    \"\"\"\n",
    "    # choose the input and output variables\n",
    "    plt.scatter(actual, np.abs(actual-predictions))\n",
    "    plt.xlabel('Mosquito target')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Scatterplot of error \\n' + case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69d8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(actual,predictions, model_type, case=''):\n",
    "    \"\"\"Prints the histogram of the actual values and the predicted values\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "        \n",
    "    model_type : str\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "    \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    if model_type == 'classification' or model_type == 'class_regression':\n",
    "        bins = np.arange(len(actual.unique())+1)-0.5\n",
    "        plt.hist(actual, bins=bins, alpha=0.5, label='actual')\n",
    "        plt.hist(predictions, bins=bins, alpha=0.5, label='prediction')\n",
    "        plt.xticks(range(len(actual.unique())))\n",
    "    else:\n",
    "        plt.hist(actual, alpha=0.5, label='actual')\n",
    "        plt.hist(predictions, alpha=0.5, label='prediction')\n",
    "    plt.legend()\n",
    "    plt.title('Histogram of actual vs predicted values \\n'+case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d02e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_per_group(actual,prediction,case=''):\n",
    "    \"\"\"Prints the error distribution plot\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "        \n",
    "    model_type : str\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "    \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "\n",
    "    \"\"\"\n",
    "    test = {'mosq_bins(t+1)':actual,'predictions':prediction}\n",
    "    test = pd.DataFrame(test)\n",
    "    test['classes'] = pd.cut(x=test['mosq_bins(t+1)'], bins=[-1, 100, 200, 300, 400, 500, np.inf],\n",
    "                      labels=['0-100', '101-200', '201-300', '301-400', '401-500', '500<'])\n",
    "    labels = test['classes'].unique().tolist()\n",
    "    labels.sort()\n",
    "    f = []\n",
    "    length = []\n",
    "    for k in labels:\n",
    "        cc = test.loc[test['classes']==k]\n",
    "        length.append(len(cc))\n",
    "        actual = cc.loc[:,'mosq_bins(t+1)']\n",
    "        predictions = cc.loc[:,'predictions']\n",
    "        f.append(mean_absolute_error(actual, predictions))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(labels,f)\n",
    "    for i, v in enumerate(f):\n",
    "        ax.text(i, v, str('%.2f'%(v)),rotation=30)\n",
    "        ax.text(i, v/2,'n = '+ str(length[i]),weight=\"bold\",ha='center')\n",
    "    plt.xlabel('group')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('MAE per group \\n'+case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3da5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_cdf(actual,prediction, case=''):\n",
    "    \"\"\"Prints the cdf of errors\n",
    "    Parameters\n",
    "    --------\n",
    "    actual : pd.Series\n",
    "        A Series with the actual class of each prediction\n",
    "        \n",
    "    predictions : pd.Series\n",
    "        A Series with the predicted class of each prediction\n",
    "        \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "\n",
    "    \"\"\"\n",
    "    error = np.abs(actual-prediction)\n",
    "    \n",
    "    a = np.sort(error.unique())\n",
    "    b = np.array(error)\n",
    "    cdf = np.zeros(len(a))\n",
    "    for k, val in enumerate(a):\n",
    "        mask_d = b <= val\n",
    "        cdf[k] = mask_d.sum()/ len(b)\n",
    "    plt.plot(a,cdf)\n",
    "    plt.grid()\n",
    "    plt.xlabel('abs(error)')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.title('CDF of error \\n' + case)\n",
    "    plt.show() \n",
    "    \n",
    "    b = np.sort(error)\n",
    "    a = np.arange(1,len(error)+1) \n",
    "    cdf = np.zeros(len(a))\n",
    "    for k, val in enumerate(b):\n",
    "        cdf[k] = b[k]\n",
    "    plt.plot(a,cdf)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of samples')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('CDF of error \\n' + case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(train, test, threshold=3):\n",
    "    \"\"\"Calculates the perfomance of the model on train and test set\n",
    "    Parameters\n",
    "    --------\n",
    "    train : Dataframe\n",
    "        A Dataframe with the actual and the predicted values on the train set\n",
    "        \n",
    "    test : Dataframe\n",
    "        A Dataframe with the actual and the predicted values on the train set\n",
    "        \n",
    "    threshold: int, optional\n",
    "        A threshold to calculate percentage of error < threshold (default= 3)\n",
    "\n",
    "    \"\"\"    \n",
    "    print('MAE on train set: ', mean_absolute_error(train['actual'], train['prediction']))\n",
    "\n",
    "    print('min prediction:',min(train['prediction']))\n",
    "    print('max prediction:',max(train['prediction']))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print('MAE on test set: ', mean_absolute_error(test['actual'], test['prediction']))\n",
    "    perc = ((np.abs(np.array(test['actual'])-np.array(test['prediction'])) < (threshold+0.5)).mean())*100\n",
    "    print('Error <= '+str(threshold)+':',\"%.2f\"%perc,'%')\n",
    "\n",
    "    print('min prediction:',min(test['prediction']))\n",
    "    print('max prediction:',max(test['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ab557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_plots(test, model_type, case=''):\n",
    "    \"\"\"Prints plots about the performance of the model on the test set\n",
    "    \n",
    "    Parameters\n",
    "    --------        \n",
    "    test : Dataframe\n",
    "        A Dataframe with the actual and the predicted values on the train set\n",
    "    \n",
    "    model_type : str\n",
    "        Could be 'class_regression' or 'mosquito_regression' or 'classification'\n",
    "        \n",
    "    case: str, optional\n",
    "        Title of the plot (default= '')\n",
    "    \"\"\"\n",
    "    if model_type == 'classification' or model_type == 'class_regression':\n",
    "        plot_error_per_class(test['actual'],test['prediction'], case)\n",
    "    else:\n",
    "        plot_error_per_group(test['actual'],test['prediction'], case)\n",
    "        error_cdf(test['actual'],test['prediction'], case)\n",
    "    scatter_plot_error(test['actual'],test['prediction'], case)\n",
    "    plot_error_dist(test['actual'],test['prediction'], model_type, case)\n",
    "    plot_hist(test['actual'],test['prediction'], model_type, case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
